name: TEST - Policy and Validation

# 🧪 TEST Phase Documentation
# 
# Purpose: Comprehensive testing and validation of infrastructure and policies
# 
# Triggers:
#   - Successful BUILD workflow completion (workflow_run)
#   - Manual workflow dispatch
#
# Duration: ~10-15 minutes  
# Dependencies: Successful BUILD workflow execution
# 
# Jobs:
#   1. test-gate: Validates BUILD workflow success before proceeding
#   2. test-info: Generate test metadata and artifact resolution
#   3. development-health-check: Validate development environment health
#   4. unit-tests: Infrastructure module unit tests (parallelized)
#   5. policy-validation: Security and compliance policy validation
#   6. usability-testing: End-to-end functionality validation
#   7. test-summary: Consolidate all test results and status
# 
# Outputs:
#   - Test results and reports
#   - Policy validation status
#   - Environment health status
#   - Consolidated test summary for DEPLOY phase
#
# Success Criteria:
#   - All unit tests pass for required modules
#   - Policy validation completes successfully
#   - Development environment health checks pass (when applicable)
#   - No critical security policy violations detected

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment for testing'
        required: false
        type: choice
        options:
          - dev
          - staging
          - prod
        default: dev
      build_id:
        description: 'Build ID to test (optional - will create new if not provided)'
        required: false
        type: string
      skip_build_check:
        description: 'Skip BUILD workflow dependency check'
        required: false
        default: false
        type: boolean
  workflow_run:
    workflows: ["BUILD - Infrastructure and Website Preparation"]
    types: [completed]
    branches: 
      - main
      - 'feature/*'
      - 'bugfix/*'
      - 'hotfix/*'

# OIDC Authentication Permissions for AWS Integration
# id-token: write - CRITICAL: Enables OIDC token generation for AWS authentication
# contents: read - Repository access for workflow execution
# security-events: write - Security scan results integration
# pull-requests: write - Test results and comments on pull requests
permissions:
  id-token: write
  contents: read
  security-events: write
  pull-requests: write

# Global workflow configuration
defaults:
  run:
    shell: bash

env:
  # Shell and terminal configuration
  TERM: xterm-256color
  FORCE_COLOR: "1"
  # AWS configuration
  AWS_DEFAULT_REGION: us-east-1
  AWS_MAX_ATTEMPTS: 3
  # Terraform/OpenTofu configuration
  TF_IN_AUTOMATION: true
  TF_CLI_ARGS: "-no-color"

concurrency:
  group: test-${{ github.ref }}
  cancel-in-progress: true

# Only run TEST if BUILD workflow completed successfully
# This prevents resource waste and ensures proper BUILD → TEST → DEPLOY flow
run-name: |
  ${{ 
    github.event_name == 'workflow_run' && github.event.workflow_run.conclusion != 'success' && 
    format('TEST - Skipped (BUILD {0})', github.event.workflow_run.conclusion) || 
    'TEST - Policy and Validation' 
  }}

jobs:
  # Gate job to validate BUILD workflow success before running any tests
  test-gate:
    name: Validate BUILD Success
    runs-on: ubuntu-latest
    timeout-minutes: 2
    if: |
      github.event_name != 'workflow_run' || 
      github.event.workflow_run.conclusion == 'success'
    outputs:
      should_run_tests: ${{ steps.validate.outputs.should_run_tests }}
      build_status: ${{ steps.validate.outputs.build_status }}
    steps:
      - name: Validate BUILD Workflow Success
        id: validate
        run: |
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            BUILD_CONCLUSION="${{ github.event.workflow_run.conclusion }}"
            if [ "$BUILD_CONCLUSION" = "success" ]; then
              echo "✅ BUILD workflow completed successfully - proceeding with TEST"
              echo "should_run_tests=true" >> $GITHUB_OUTPUT
              echo "build_status=success" >> $GITHUB_OUTPUT
            else
              echo "❌ BUILD workflow failed with conclusion: $BUILD_CONCLUSION"
              echo "🛑 Stopping TEST workflow execution"
              echo "should_run_tests=false" >> $GITHUB_OUTPUT
              echo "build_status=$BUILD_CONCLUSION" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            echo "✅ Manual trigger - proceeding with TEST"
            echo "should_run_tests=true" >> $GITHUB_OUTPUT
            echo "build_status=manual" >> $GITHUB_OUTPUT
          fi
  test-info:
    name: Test Information
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test-gate]
    if: needs.test-gate.outputs.should_run_tests == 'true'
    outputs:
      test_id: ${{ steps.generate-id.outputs.test_id }}
      build_id: ${{ steps.generate-id.outputs.build_id }}
      build_success: ${{ steps.check-build.outputs.build_success }}
      resolved_environment: ${{ steps.resolve-environment.outputs.resolved_environment }}
      has_tf_changes: ${{ steps.detect-changes.outputs.has_tf_changes }}
      has_test_changes: ${{ steps.detect-changes.outputs.has_test_changes }}
      skip_tests: ${{ steps.detect-changes.outputs.skip_tests }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check BUILD Status
        id: check-build
        if: github.event_name == 'workflow_run' && github.event.inputs.skip_build_check != 'true'
        run: |
          if [ "${{ github.event.workflow_run.conclusion }}" != "success" ]; then
            echo "BUILD workflow failed - cannot proceed with TEST"
            echo "build_success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "BUILD workflow passed - proceeding with TEST"
          echo "build_success=true" >> $GITHUB_OUTPUT

      - name: Resolve Environment
        id: resolve-environment
        run: |
          if [ -n "${{ github.event.inputs.environment }}" ]; then
            RESOLVED_ENV="${{ github.event.inputs.environment }}"
            ENV_SOURCE="Manual Input"
          elif [ -n "${{ vars.DEFAULT_ENVIRONMENT }}" ]; then
            RESOLVED_ENV="${{ vars.DEFAULT_ENVIRONMENT }}"
            ENV_SOURCE="Repository Variable"
          else
            RESOLVED_ENV="dev"
            ENV_SOURCE="Hardcoded Fallback"
          fi
          
          echo "resolved_environment=$RESOLVED_ENV" >> $GITHUB_OUTPUT
          echo "environment_source=$ENV_SOURCE" >> $GITHUB_OUTPUT
          
          echo "🧪 **TEST Phase Started**" >> $GITHUB_STEP_SUMMARY
          echo "**Environment**: $RESOLVED_ENV (Source: $ENV_SOURCE)" >> $GITHUB_STEP_SUMMARY

      - name: Generate Test ID
        id: generate-id
        run: |
          if [ -n "${{ github.event.inputs.build_id }}" ]; then
            BUILD_ID="${{ github.event.inputs.build_id }}"
          elif [ "${{ github.event_name }}" = "workflow_run" ]; then
            BUILD_ID="build-${{ github.event.workflow_run.id }}"
          else
            BUILD_ID="build-${{ github.run_id }}-auto"
          fi
          
          TEST_ID="test-${{ github.run_id }}-${{ github.run_attempt }}"
          
          echo "build_id=$BUILD_ID" >> $GITHUB_OUTPUT
          echo "test_id=$TEST_ID" >> $GITHUB_OUTPUT
          
          echo "**Test ID**: $TEST_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Build ID**: $BUILD_ID" >> $GITHUB_STEP_SUMMARY

      - name: Detect Changes
        id: detect-changes
        run: |
          echo "🔍 **Analyzing Changes for Test Optimization**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Get changed files
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            # For workflow_run events, get files from the triggering commit
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD 2>/dev/null || echo "")
          else
            # For other events, compare with main branch
            CHANGED_FILES=$(git diff --name-only origin/main...HEAD 2>/dev/null || echo "")
          fi
          
          echo "Changed files: $CHANGED_FILES"
          
          # Categorize changes
          HAS_TF_CHANGES=0
          HAS_TEST_CHANGES=0
          HAS_CODE_CHANGES=0
          SKIP_TESTS=0
          
          if [ -n "$CHANGED_FILES" ]; then
            # Check for infrastructure changes
            if echo "$CHANGED_FILES" | grep -qE '^terraform/.*\.(tf|tfvars)$'; then
              HAS_TF_CHANGES=1
            fi
            
            # Also consider workflow changes as infrastructure changes for policy validation
            if echo "$CHANGED_FILES" | grep -qE '^\.github/workflows/(build|test|deploy)\.yml$'; then
              HAS_TF_CHANGES=1
            fi
            
            # Check for test changes  
            if echo "$CHANGED_FILES" | grep -qE '^test/'; then
              HAS_TEST_CHANGES=1
            fi
            
            # Check for code/config changes
            if echo "$CHANGED_FILES" | grep -qE '^(src/|\.github/workflows/|\.github/actions/)'; then
              HAS_CODE_CHANGES=1
            fi
            
            # Skip tests if only documentation changes
            if echo "$CHANGED_FILES" | grep -qE '\.(md|txt|rst)$' && \
               ! echo "$CHANGED_FILES" | grep -qvE '\.(md|txt|rst)$'; then
              SKIP_TESTS=1
              echo "⚡ Only documentation changes detected - skipping tests" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Skip tests if only badge changes
            if echo "$CHANGED_FILES" | grep -qE '^\.github/badges/' && \
               ! echo "$CHANGED_FILES" | grep -qvE '^\.github/badges/'; then
              SKIP_TESTS=1
              echo "🏷️ Only badge changes detected - skipping tests" >> $GITHUB_STEP_SUMMARY
            fi
          else
            # No changes detected, run all tests
            HAS_TF_CHANGES=1
            HAS_TEST_CHANGES=1
            HAS_CODE_CHANGES=1
          fi
          
          echo "has_tf_changes=$HAS_TF_CHANGES" >> $GITHUB_OUTPUT
          echo "has_test_changes=$HAS_TEST_CHANGES" >> $GITHUB_OUTPUT
          echo "skip_tests=$SKIP_TESTS" >> $GITHUB_OUTPUT
          
          echo "**Infrastructure Changes**: $HAS_TF_CHANGES" >> $GITHUB_STEP_SUMMARY
          echo "**Test Changes**: $HAS_TEST_CHANGES" >> $GITHUB_STEP_SUMMARY
          echo "**Skip Tests**: $SKIP_TESTS" >> $GITHUB_STEP_SUMMARY

      - name: Download BUILD Artifacts
        if: github.event_name == 'workflow_run'
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16 # v4.1.8
        with:
          run-id: ${{ github.event.workflow_run.id }}
          merge-multiple: true
        continue-on-error: true

  development-health-check:
    name: Development Environment Health Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [test-gate, test-info]
    if: |
      needs.test-gate.outputs.should_run_tests == 'true' &&
      (needs.test-info.outputs.resolved_environment == 'staging' ||
      needs.test-info.outputs.resolved_environment == 'prod')
    outputs:
      dev_health_status: ${{ steps.health-check.outputs.health_status }}
      dev_site_url: ${{ steps.health-check.outputs.site_url }}
    steps:
      - name: Check Development Environment Health
        id: health-check
        run: |
          echo "## 🩺 Development Environment Health Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Query latest development deployment status via GitHub API
          DEV_DEPLOYMENTS=$(gh api repos/${{ github.repository }}/deployments \
            --paginate \
            --jq '.[] | select(.environment=="development") | {id: .id, ref: .ref, created_at: .created_at}' | \
            jq -s 'sort_by(.created_at) | reverse | .[0]')
          
          if [[ "$DEV_DEPLOYMENTS" == "null" ]] || [[ -z "$DEV_DEPLOYMENTS" ]]; then
            echo "❌ No development deployments found" >> $GITHUB_STEP_SUMMARY
            echo "   Staging/Production deployments require healthy development environment" >> $GITHUB_STEP_SUMMARY
            echo "health_status=no_deployments" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Get the latest deployment ID and check its status
          LATEST_DEV_ID=$(echo "$DEV_DEPLOYMENTS" | jq -r '.id')
          DEV_STATUS=$(gh api repos/${{ github.repository }}/deployments/$LATEST_DEV_ID/statuses \
            --jq '.[0].state // "unknown"')
          
          echo "**Latest Development Deployment**: $LATEST_DEV_ID" >> $GITHUB_STEP_SUMMARY
          echo "**Status**: $DEV_STATUS" >> $GITHUB_STEP_SUMMARY
          
          if [[ "$DEV_STATUS" == "success" ]]; then
            echo "✅ Development environment is healthy" >> $GITHUB_STEP_SUMMARY
            echo "   Staging deployment can proceed" >> $GITHUB_STEP_SUMMARY
            echo "health_status=healthy" >> $GITHUB_OUTPUT
          else
            echo "❌ Development environment is unhealthy (status: $DEV_STATUS)" >> $GITHUB_STEP_SUMMARY
            echo "   Staging deployment blocked until development is fixed" >> $GITHUB_STEP_SUMMARY
            echo "   Please fix development environment and re-run tests" >> $GITHUB_STEP_SUMMARY
            echo "health_status=$DEV_STATUS" >> $GITHUB_OUTPUT
            exit 1
          fi

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test-gate, test-info, development-health-check]
    if: |
      always() && 
      needs.test-gate.outputs.should_run_tests == 'true' &&
      needs.test-info.outputs.skip_tests != '1' &&
      (needs.development-health-check.result == 'success' || needs.development-health-check.result == 'skipped')
    strategy:
      matrix:
        module: [s3, cloudfront, waf, iam, monitoring]
      fail-fast: false
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Cache Test Dependencies
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: |
            ~/.cache/terraform
            test/unit/test-results
          key: test-deps-${{ runner.os }}-${{ matrix.module }}-${{ hashFiles('test/unit/*.sh') }}
          restore-keys: |
            test-deps-${{ runner.os }}-${{ matrix.module }}-

      - name: Setup Infrastructure
        uses: ./.github/actions/setup-infrastructure
        with:
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}
          aws-role: ${{ secrets.AWS_ASSUME_ROLE }}

      - name: Install Dependencies
        run: |
          echo "## 🧪 Unit Tests for ${{ matrix.module }} Module" >> $GITHUB_STEP_SUMMARY
          echo "**Test ID**: ${{ needs.test-info.outputs.test_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Build ID**: ${{ needs.test-info.outputs.build_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Environment**: ${{ needs.test-info.outputs.resolved_environment }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Install required dependencies
          sudo apt-get update -qq
          sudo apt-get install -y jq bc curl

      - name: Run Unit Tests
        working-directory: test/unit
        env:
          AWS_DEFAULT_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
          AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
          TF_VAR_environment: "unit-test-${{ github.run_number }}-${{ matrix.module }}"
          TF_VAR_project_name: "static-site-test"
          TEST_ENV_PREFIX: "unit-test-${{ github.run_number }}"
        run: |
          echo "🧪 Running unit tests for ${{ matrix.module }} module..."
          echo "Environment: $TF_VAR_environment"
          echo "AWS Region: $AWS_REGION"
          echo ""
          
          # Run the appropriate test script (all test files now exist)
          case "${{ matrix.module }}" in
            "s3")
              timeout 20m ./test-s3.sh | tee test-output-${{ matrix.module }}.log
              ;;
            "cloudfront")
              timeout 20m ./test-cloudfront.sh | tee test-output-${{ matrix.module }}.log
              ;;
            "waf")
              timeout 20m ./test-waf.sh | tee test-output-${{ matrix.module }}.log
              ;;
            "iam")
              timeout 20m ./test-iam.sh | tee test-output-${{ matrix.module }}.log
              ;;
            "monitoring")
              timeout 20m ./test-monitoring.sh | tee test-output-${{ matrix.module }}.log
              ;;
            *)
              echo "❌ No test script found for module: ${{ matrix.module }}" | tee test-output-${{ matrix.module }}.log
              exit 1
              ;;
          esac
          
          # Enhanced test result checking
          if [ -f "test-status.txt" ]; then
            TEST_STATUS=$(cat test-status.txt)
            if [ "$TEST_STATUS" = "All tests passed!" ]; then
              echo "✅ All tests passed for ${{ matrix.module }}" >> $GITHUB_STEP_SUMMARY
              
              # Add test summary to GitHub Actions summary if available
              if [ -f "${{ matrix.module }}-test-summary.md" ]; then
                echo "" >> $GITHUB_STEP_SUMMARY
                cat "${{ matrix.module }}-test-summary.md" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "❌ Some tests failed for ${{ matrix.module }}" >> $GITHUB_STEP_SUMMARY
              
              # Add detailed failure information
              if [ -f "${{ matrix.module }}-test-summary.md" ]; then
                echo "" >> $GITHUB_STEP_SUMMARY
                cat "${{ matrix.module }}-test-summary.md" >> $GITHUB_STEP_SUMMARY
              else
                echo '```' >> $GITHUB_STEP_SUMMARY
                tail -20 test-output-${{ matrix.module }}.log >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
              fi
              exit 1
            fi
          else
            echo "⚠️ Test status unclear for ${{ matrix.module }}" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -20 test-output-${{ matrix.module }}.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            exit 1
          fi

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: ${{ needs.test-info.outputs.test_id }}-unit-test-${{ matrix.module }}
          path: |
            test/unit/test-output-${{ matrix.module }}.log
            test/unit/${{ matrix.module }}-test-results.json
            test/unit/${{ matrix.module }}-test-summary.md
            test/unit/test-status.txt
          retention-days: 30

  policy-validation:
    name: Policy Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test-gate, test-info]
    if: needs.test-gate.outputs.should_run_tests == 'true' && needs.test-info.outputs.skip_tests != '1' && needs.test-info.outputs.has_tf_changes == '1'
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Infrastructure
        uses: ./.github/actions/setup-infrastructure
        with:
          aws-region: ${{ vars.AWS_REGION || 'us-east-2' }}
          aws-role: ${{ secrets.AWS_ASSUME_ROLE }}

      - name: Install OPA and Conftest
        run: |
          echo "## 🛡️ Policy Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "**Test ID**: ${{ needs.test-info.outputs.test_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Build ID**: ${{ needs.test-info.outputs.build_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Install OPA with checksum verification
          OPA_VERSION="v0.57.0"
          curl -L -o opa "https://openpolicyagent.org/downloads/${OPA_VERSION}/opa_linux_amd64_static"
          chmod +x opa
          sudo mv opa /usr/local/bin/
          
          # Install Conftest with checksum verification
          CONFTEST_VERSION="v0.46.0"
          curl -L -o conftest.tar.gz "https://github.com/open-policy-agent/conftest/releases/download/${CONFTEST_VERSION}/conftest_${CONFTEST_VERSION#v}_Linux_x86_64.tar.gz"
          tar xzf conftest.tar.gz
          sudo mv conftest /usr/local/bin/
          rm conftest.tar.gz

      - name: Create Security Policies
        run: |
          mkdir -p policies
          
          # Static website specific security policies
          cat > policies/static-website-security.rego << 'EOF'
          package terraform.static_website.security
          
          # S3 bucket must be encrypted
          deny[msg] {
            input.resource_changes[_].type == "aws_s3_bucket"
            bucket := input.resource_changes[_]
            not bucket.change.after.server_side_encryption_configuration
            msg := "S3 buckets must have server-side encryption enabled"
          }
          
          # S3 bucket must block public access
          deny[msg] {
            input.resource_changes[_].type == "aws_s3_bucket_public_access_block"
            pab := input.resource_changes[_]
            not pab.change.after.block_public_acls
            msg := "S3 bucket must block public ACLs"
          }
          
          # CloudFront must use HTTPS only
          deny[msg] {
            input.resource_changes[_].type == "aws_cloudfront_distribution"
            cf := input.resource_changes[_]
            cf.change.after.default_cache_behavior[_].viewer_protocol_policy != "redirect-to-https"
            msg := "CloudFront distribution must redirect HTTP to HTTPS"
          }
          
          # WAF must be enabled for CloudFront
          deny[msg] {
            input.resource_changes[_].type == "aws_cloudfront_distribution"
            cf := input.resource_changes[_]
            not cf.change.after.web_acl_id
            msg := "CloudFront distribution must have WAF enabled"
          }
          
          # IAM roles must follow least privilege
          warn[msg] {
            input.resource_changes[_].type == "aws_iam_role_policy_attachment"
            attachment := input.resource_changes[_]
            contains(attachment.change.after.policy_arn, "PowerUserAccess")
            msg := "IAM role should not use PowerUserAccess - follow least privilege principle"
          }
          EOF
          
          # Compliance policies
          cat > policies/static-website-compliance.rego << 'EOF'
          package terraform.static_website.compliance
          
          # Resources must have required tags
          required_tags := ["Environment", "Project", "ManagedBy"]
          
          warn[msg] {
            resource := input.resource_changes[_]
            resource.type == "aws_s3_bucket"
            tag := required_tags[_]
            not resource.change.after.tags[tag]
            msg := sprintf("Resource %s should have tag: %s", [resource.type, tag])
          }
          
          warn[msg] {
            resource := input.resource_changes[_]
            resource.type == "aws_cloudfront_distribution"
            tag := required_tags[_]
            not resource.change.after.tags[tag]
            msg := sprintf("Resource %s should have tag: %s", [resource.type, tag])
          }
          
          # S3 bucket names must follow naming convention
          warn[msg] {
            input.resource_changes[_].type == "aws_s3_bucket"
            bucket := input.resource_changes[_]
            not regex.match("^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$", bucket.change.after.bucket)
            msg := "S3 bucket name must follow DNS-compliant naming convention"
          }
          
          # CloudFront must have security headers
          warn[msg] {
            input.resource_changes[_].type == "aws_cloudfront_function"
            cf_function := input.resource_changes[_]
            not contains(cf_function.change.after.code, "x-content-type-options")
            msg := "CloudFront function should include security headers"
          }
          EOF

      - name: Generate Plans and Validate
        id: policy
        working-directory: terraform
        run: |
          echo "### 🛡️ Policy Validation Results" >> $GITHUB_STEP_SUMMARY
          
          validation_errors=0
          validation_warnings=0
          
          # Generate plan for policy validation with required variables
          export TF_VAR_environment="dev"
          export TF_VAR_project_name="static-site-policy-test"
          export TF_VAR_github_repository="test/test"
          export TF_VAR_aws_region="us-east-2"
          export TF_VAR_alert_email_addresses='["test@example.com"]'
          export TF_VAR_use_existing_iam_role="false"
          
          echo "Generating Terraform plan for policy validation..."
          echo "Using test variables:" >> $GITHUB_STEP_SUMMARY
          echo "- Environment: $TF_VAR_environment" >> $GITHUB_STEP_SUMMARY
          echo "- Project: $TF_VAR_project_name" >> $GITHUB_STEP_SUMMARY  
          echo "- Repository: $TF_VAR_github_repository" >> $GITHUB_STEP_SUMMARY
          echo "- AWS Region: $TF_VAR_aws_region" >> $GITHUB_STEP_SUMMARY
          
          # Create temporary directory for policy validation (no backend needed)
          mkdir -p temp-validation
          cp *.tf temp-validation/ 2>/dev/null || true
          cp -r modules temp-validation/ 2>/dev/null || true
          cp -r ../policies temp-validation/ 2>/dev/null || true
          rm -f temp-validation/backend.tf  # Remove backend configuration
          
          cd temp-validation
          if tofu init; then
            echo "✅ Terraform initialized successfully (temp directory)" >> $GITHUB_STEP_SUMMARY
            if tofu plan -out=plan.tfplan; then
              tofu show -json plan.tfplan > plan.json
              
              echo "#### Security Policy Results" >> $GITHUB_STEP_SUMMARY
              
              if conftest verify --policy policies/static-website-security.rego plan.json > security_results.txt 2>&1; then
                echo "✅ Security policies passed" >> $GITHUB_STEP_SUMMARY
              else
                echo "❌ Security policy violations found:" >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                head -10 security_results.txt >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                validation_errors=$((validation_errors + 1))
              fi
              
              echo "#### Compliance Policy Results" >> $GITHUB_STEP_SUMMARY
              
              if conftest verify --policy policies/static-website-compliance.rego plan.json > compliance_results.txt 2>&1; then
                echo "✅ Compliance policies passed" >> $GITHUB_STEP_SUMMARY
              else
                echo "⚠️ Compliance policy warnings:" >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                head -5 compliance_results.txt >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                validation_warnings=$((validation_warnings + 1))
              fi
              
            else
              echo "❌ Could not generate Terraform plan" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "Plan error details:" >> $GITHUB_STEP_SUMMARY
              tofu plan -out=plan.tfplan 2>&1 | tail -20 >> $GITHUB_STEP_SUMMARY || echo "Could not capture plan output" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              validation_errors=$((validation_errors + 1))
            fi
          else
            echo "❌ Could not initialize Terraform" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "Init error details:" >> $GITHUB_STEP_SUMMARY
            tofu init 2>&1 | tail -20 >> $GITHUB_STEP_SUMMARY || echo "Could not capture init output" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            validation_errors=$((validation_errors + 1))
          fi
          
          # Return to original directory and cleanup
          cd ..
          rm -rf temp-validation
          
          echo "validation_errors=$validation_errors" >> $GITHUB_OUTPUT
          echo "validation_warnings=$validation_warnings" >> $GITHUB_OUTPUT
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📊 Policy Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Security errors**: $validation_errors" >> $GITHUB_STEP_SUMMARY
          echo "- **Compliance warnings**: $validation_warnings" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ $validation_errors -gt 0 ]; then
            echo "❌ Policy validation failed with $validation_errors security errors" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "✅ Policy validation passed (warnings: $validation_warnings)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Policy Results
        if: always()
        uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: ${{ needs.test-info.outputs.test_id }}-policy-validation
          path: |
            policies/
            terraform/plan.json
            terraform/security_results.txt
            terraform/compliance_results.txt
          retention-days: 30

  usability-testing:
    name: Usability Testing
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test-gate, test-info, development-health-check]
    if: |
      always() && 
      needs.test-gate.outputs.should_run_tests == 'true' &&
      needs.test-info.outputs.resolved_environment == 'staging' &&
      (needs.development-health-check.result == 'success')
    outputs:
      usability_test_status: ${{ steps.usability-tests.outputs.test_status }}
      total_tests: ${{ steps.usability-tests.outputs.total_tests }}
      passed_tests: ${{ steps.usability-tests.outputs.passed_tests }}
      failed_tests: ${{ steps.usability-tests.outputs.failed_tests }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Test Environment
        run: |
          # Install required tools for usability testing
          sudo apt-get update
          sudo apt-get install -y curl jq bc openssl
          
          # Make test scripts executable
          chmod +x test/usability/*.sh

      - name: Run Staging Usability Tests
        id: usability-tests
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "## 🧪 Staging Usability Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Get staging site URL from deployment records or construct it
          STAGING_URL=""
          
          # Try to get from latest staging deployment
          STAGING_DEPLOYMENTS=$(gh api repos/${{ github.repository }}/deployments \
            --paginate \
            --jq '.[] | select(.environment=="staging") | {id: .id, ref: .ref, created_at: .created_at}' | \
            jq -s 'sort_by(.created_at) | reverse | .[0]' || echo "{}")
          
          if [[ "$STAGING_DEPLOYMENTS" != "{}" ]] && [[ -n "$STAGING_DEPLOYMENTS" ]]; then
            echo "📍 Found staging deployment records" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No staging deployment records found, using constructed URL" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Run comprehensive usability tests
          if test/usability/staging-usability-tests.sh; then
            echo "✅ All usability tests passed" >> $GITHUB_STEP_SUMMARY
            echo "test_status=passed" >> $GITHUB_OUTPUT
            TEST_EXIT_CODE=0
          else
            echo "❌ Some usability tests failed" >> $GITHUB_STEP_SUMMARY  
            echo "test_status=failed" >> $GITHUB_OUTPUT
            TEST_EXIT_CODE=1
          fi
          
          # Export results if available
          RESULTS_FILE=$(find test/usability/test-results -name "staging-usability-results-*.json" -type f | head -1)
          if [[ -f "$RESULTS_FILE" ]]; then
            echo "total_tests=$(jq -r '.summary.total_tests // 0' "$RESULTS_FILE")" >> $GITHUB_OUTPUT
            echo "passed_tests=$(jq -r '.summary.passed_tests // 0' "$RESULTS_FILE")" >> $GITHUB_OUTPUT
            echo "failed_tests=$(jq -r '.summary.failed_tests // 0' "$RESULTS_FILE")" >> $GITHUB_OUTPUT
            
            # Add detailed results to summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Test Results:**" >> $GITHUB_STEP_SUMMARY
            echo "- Total: $(jq -r '.summary.total_tests // 0' "$RESULTS_FILE")" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: $(jq -r '.summary.passed_tests // 0' "$RESULTS_FILE")" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: $(jq -r '.summary.failed_tests // 0' "$RESULTS_FILE")" >> $GITHUB_STEP_SUMMARY
          fi
          
          exit $TEST_EXIT_CODE

      - name: Upload Usability Test Results
        if: always()
        uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: ${{ needs.test-info.outputs.test_id }}-usability-tests
          path: |
            test/usability/test-results/
            test/usability/test-output.log
          retention-days: 30

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [test-gate, test-info, development-health-check, unit-tests, policy-validation, usability-testing]
    if: always()
    steps:
      - name: Download Unit Test Results
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16 # v4.1.8
        with:
          pattern: ${{ needs.test-info.outputs.test_id }}-unit-test-*
          merge-multiple: true
        continue-on-error: true

      - name: Generate Test Summary
        run: |
          echo "## 🧪 TEST Phase Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test ID**: ${{ needs.test-info.outputs.test_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Build ID**: ${{ needs.test-info.outputs.build_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Check if tests were skipped
          if [ "${{ needs.test-info.outputs.skip_tests }}" = "1" ]; then
            echo "⚡ **Tests Skipped**: Only documentation changes detected" >> $GITHUB_STEP_SUMMARY
            echo "✅ **TEST Phase Successful** (no testing required)" >> $GITHUB_STEP_SUMMARY
            echo "➡️ **Next Phase**: DEPLOY (ready for deployment)" >> $GITHUB_STEP_SUMMARY
          else
            echo "### 🔍 Test Results Summary" >> $GITHUB_STEP_SUMMARY
            echo "- **Unit Tests**: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Policy Validation**: ${{ needs.policy-validation.result }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Usability Testing**: ${{ needs.usability-testing.result }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Final assessment
            UNIT_OK=false
            POLICY_OK=false
            USABILITY_OK=false
            
            if [ "${{ needs.unit-tests.result }}" = "success" ]; then
              UNIT_OK=true
            fi
            
            if [ "${{ needs.policy-validation.result }}" = "success" ] || [ "${{ needs.policy-validation.result }}" = "skipped" ]; then
              POLICY_OK=true
            fi
            
            if [ "${{ needs.usability-testing.result }}" = "success" ] || [ "${{ needs.usability-testing.result }}" = "skipped" ]; then
              USABILITY_OK=true
            fi
            
            if [ "$UNIT_OK" = true ] && [ "$POLICY_OK" = true ] && [ "$USABILITY_OK" = true ]; then
              echo "✅ **TEST Phase Successful**" >> $GITHUB_STEP_SUMMARY
              echo "All validation tests completed successfully" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "➡️ **Next Phase**: DEPLOY workflow will handle environment-specific deployments" >> $GITHUB_STEP_SUMMARY
              echo "- **Feature branches**: Auto-deploy to development after successful TEST" >> $GITHUB_STEP_SUMMARY
              echo "- **Pull requests**: Deploy to staging for validation" >> $GITHUB_STEP_SUMMARY
              echo "- **Main branch**: Available for production deployment" >> $GITHUB_STEP_SUMMARY
            else
              echo "❌ **TEST Phase Failed**" >> $GITHUB_STEP_SUMMARY
              echo "Please review the failed tests and fix issues before proceeding." >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Upload Test Summary
        if: always()
        uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: ${{ needs.test-info.outputs.test_id }}-test-summary
          path: |
            *-test-results.json
            *-test-summary.md
          retention-days: 30