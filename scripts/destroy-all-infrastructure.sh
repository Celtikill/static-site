#!/usr/bin/env bash
#
# Script: destroy-all-infrastructure.sh
# Purpose: Complete destruction of all AWS infrastructure created by this repository
# Author: Generated by Claude for static-site infrastructure cleanup
#
# DANGER: This script will PERMANENTLY DELETE all AWS resources created by this repository.
# This includes SINGLE-ACCOUNT and CROSS-ACCOUNT resources:
#
# CROSS-ACCOUNT RESOURCES (requires management account access):
# - GitHub Actions roles in all member accounts (dev, staging, prod)
# - Cross-account Terraform state cleanup
# - Member account closure (optional, PERMANENT for 90 days)
#
# SINGLE-ACCOUNT RESOURCES:
# - All S3 buckets and their contents (including replicas in all US regions)
# - All S3 replication configurations and intelligent tiering configs
# - All KMS keys (scheduled for deletion)
# - All IAM roles, policies, users, groups, and OIDC providers
# - All CloudFront distributions
# - All DynamoDB tables (Terraform state locks)
# - All CloudTrail trails and logs
# - All SNS topics and subscriptions
# - All CloudWatch alarms, log groups, dashboards, and composite alarms
# - All WAF Web ACLs
# - All Route53 hosted zones, health checks, and DNS records
# - All AWS Budgets and budget actions
# - All SSM Parameter Store parameters
# - All ACM certificates
# - Any orphaned EC2 resources (Elastic IPs, etc.)
#
# AWS ORGANIZATIONS RESOURCES (management account):
# - Service Control Policies (detached and deleted)
# - Organizational Units (deleted bottom-up)
# - Policy attachments to OUs and accounts
#
# Required Environment Variables:
#   - AWS_DEFAULT_REGION: AWS region (default: us-east-1)
# Optional Environment Variables:
#   - FORCE_DESTROY: Set to 'true' to skip confirmation prompts
#   - DRY_RUN: Set to 'true' to see what would be destroyed without doing it
#   - ACCOUNT_FILTER: Comma-separated list of account IDs to limit destruction to
#   - INCLUDE_CROSS_ACCOUNT: Set to 'false' to disable cross-account destruction
#   - CLOSE_MEMBER_ACCOUNTS: Set to 'true' to enable member account closure
#   - CLEANUP_TERRAFORM_STATE: Set to 'false' to disable Terraform state cleanup
#
# Usage:
#   # Dry run mode (recommended first step)
#   ./destroy-all-infrastructure.sh --dry-run
#
#   # Complete infrastructure destruction with cross-account cleanup
#   ./destroy-all-infrastructure.sh --force
#
#   # Destroy specific accounts only
#   ./destroy-all-infrastructure.sh --account-filter "822529998967,927588814642" --dry-run
#
#   # Full destruction including member account closure (EXTREME)
#   ./destroy-all-infrastructure.sh --force --close-accounts
#
#   # Single account cleanup only (no cross-account features)
#   ./destroy-all-infrastructure.sh --no-cross-account --no-terraform-cleanup
#
# Exit Codes:
#   0 - Success (all resources destroyed)
#   1 - General failure
#   2 - User cancelled operation
#   3 - AWS CLI not configured
#   4 - Missing required permissions

set -euo pipefail

# Constants and Configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly SCRIPT_NAME="$(basename "${BASH_SOURCE[0]}")"
readonly LOG_FILE="/tmp/destroy-infrastructure-$(date +%Y%m%d-%H%M%S).log"

# Default values
: "${AWS_DEFAULT_REGION:=us-east-1}"
: "${FORCE_DESTROY:=false}"
: "${DRY_RUN:=false}"
: "${ACCOUNT_FILTER:=}"
: "${INCLUDE_CROSS_ACCOUNT:=true}"
: "${CLOSE_MEMBER_ACCOUNTS:=false}"
: "${CLEANUP_TERRAFORM_STATE:=true}"

# Colors for output (if terminal supports it)
if [[ -t 1 ]]; then
    readonly RED='\033[0;31m'
    readonly GREEN='\033[0;32m'
    readonly YELLOW='\033[1;33m'
    readonly BLUE='\033[0;34m'
    readonly BOLD='\033[1m'
    readonly NC='\033[0m' # No Color
else
    readonly RED=''
    readonly GREEN=''
    readonly YELLOW=''
    readonly BLUE=''
    readonly BOLD=''
    readonly NC=''
fi

# Project-specific resource patterns
readonly PROJECT_PATTERNS=(
    "static-site"
    "StaticSite"
    "terraform-state"
    "GitHubActions"
    "cloudtrail-logs"
)

# AWS Organization Account IDs (from organization-management.yml)
readonly MEMBER_ACCOUNT_IDS=(
    "822529998967"  # dev
    "927588814642"  # staging
    "546274483801"  # prod
)

readonly MANAGEMENT_ACCOUNT_ID="223938610551"
readonly EXTERNAL_ID="github-actions-static-site"

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "$LOG_FILE"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$LOG_FILE"
}

log_action() {
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${YELLOW}[DRY RUN]${NC} Would $1" | tee -a "$LOG_FILE"
    else
        echo -e "${BOLD}[ACTION]${NC} $1" | tee -a "$LOG_FILE"
    fi
}

# Confirmation function
confirm_destruction() {
    local resource_type="$1"
    local resource_name="$2"

    if [[ "$FORCE_DESTROY" == "true" ]] || [[ "$DRY_RUN" == "true" ]]; then
        return 0
    fi

    echo -e "${RED}${BOLD}DANGER:${NC} About to destroy ${YELLOW}$resource_type${NC}: ${BOLD}$resource_name${NC}"

    # Add timeout for non-interactive environments
    local confirmation
    if read -t 30 -p "Are you sure? (type 'DELETE' to confirm): " confirmation; then
        if [[ "$confirmation" != "DELETE" ]]; then
            log_warn "Skipping $resource_type: $resource_name"
            return 1
        fi
    else
        log_warn "Timeout waiting for confirmation - skipping $resource_type: $resource_name"
        return 1
    fi
    return 0
}

# Check if resource matches project patterns
matches_project() {
    local resource_name="$1"
    local pattern

    # Handle null or empty resource names
    if [[ -z "$resource_name" ]] || [[ "$resource_name" == "null" ]]; then
        return 1
    fi

    for pattern in "${PROJECT_PATTERNS[@]}"; do
        if [[ "$resource_name" == *"$pattern"* ]]; then
            return 0
        fi
    done
    return 1
}

# Check account filter
check_account_filter() {
    local account_id="$1"

    if [[ -z "$ACCOUNT_FILTER" ]]; then
        return 0  # No filter, allow all
    fi

    IFS=',' read -ra ACCOUNTS <<< "$ACCOUNT_FILTER"
    for allowed_account in "${ACCOUNTS[@]}"; do
        if [[ "$account_id" == "$allowed_account" ]]; then
            return 0
        fi
    done
    return 1
}

# Get all US AWS regions
get_us_regions() {
    aws ec2 describe-regions \
        --query 'Regions[?starts_with(RegionName, `us-`)].RegionName' \
        --output text 2>/dev/null || echo "us-east-1 us-east-2 us-west-1 us-west-2"
}

# Execute function across all US regions
execute_in_all_regions() {
    local function_name="$1"
    local regions

    log_info "Executing $function_name across all US regions..."
    regions=$(get_us_regions)

    for region in $regions; do
        log_info "Processing region: $region"
        export AWS_REGION="$region"
        "$function_name" "$region"
    done

    # Reset to default region
    export AWS_REGION="$AWS_DEFAULT_REGION"
}

# AWS CLI wrapper with error handling
aws_cmd() {
    local cmd=("$@")

    if [[ "$DRY_RUN" == "true" ]]; then
        log_action "Run: aws ${cmd[*]}"
        return 0
    fi

    # Add retry logic for transient failures
    local max_retries=3
    local retry_count=0

    while [[ $retry_count -lt $max_retries ]]; do
        if aws "${cmd[@]}" 2>>"$LOG_FILE"; then
            return 0
        fi

        ((retry_count++))
        if [[ $retry_count -lt $max_retries ]]; then
            log_warn "Command failed, retrying (${retry_count}/${max_retries}): aws ${cmd[*]}"
            sleep 2
        fi
    done

    log_error "Failed to execute after ${max_retries} attempts: aws ${cmd[*]}"
    return 1
}

# Destroy S3 buckets and contents
destroy_s3_buckets() {
    log_info "ðŸª£ Scanning for S3 buckets..."

    local buckets
    # S3 buckets are global, no need for region-specific calls
    buckets=$(AWS_DEFAULT_REGION=us-east-1 aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)

    if [[ -z "$buckets" ]]; then
        log_info "No S3 buckets found"
        return 0
    fi

    for bucket in $buckets; do
        if matches_project "$bucket"; then
            if confirm_destruction "S3 Bucket" "$bucket"; then
                log_action "Empty and delete S3 bucket: $bucket"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Remove replication configuration if exists
                    aws s3api delete-bucket-replication --bucket "$bucket" 2>/dev/null || true

                    # Remove intelligent tiering configuration if exists
                    aws s3api list-bucket-intelligent-tiering-configurations --bucket "$bucket" --query 'IntelligentTieringConfigurationList[].Id' --output text 2>/dev/null | \
                        while read -r config_id; do
                            [[ -n "$config_id" ]] && aws s3api delete-bucket-intelligent-tiering-configuration --bucket "$bucket" --id "$config_id" 2>/dev/null || true
                        done

                    # Empty bucket first (including all versions and delete markers)
                    aws s3api list-object-versions --bucket "$bucket" \
                        --query 'Versions[].{Key:Key,VersionId:VersionId}' \
                        --output text 2>/dev/null | \
                        while read -r key version_id; do
                            [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Delete delete markers
                    aws s3api list-object-versions --bucket "$bucket" \
                        --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' \
                        --output text 2>/dev/null | \
                        while read -r key version_id; do
                            [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Force empty using CLI (as backup)
                    aws s3 rm "s3://$bucket" --recursive 2>/dev/null || true

                    # Delete bucket
                    if aws s3api delete-bucket --bucket "$bucket" 2>/dev/null; then
                        log_success "Deleted S3 bucket: $bucket"
                    else
                        log_error "Failed to delete S3 bucket: $bucket"
                    fi
                fi
            fi
        fi
    done
}

# Destroy replica S3 buckets in secondary regions
destroy_replica_s3_buckets() {
    local region="${1:-us-west-2}"
    log_info "ðŸª£ Scanning for replica S3 buckets in $region..."

    # S3 buckets are global, but we need to check regional endpoints
    local buckets
    buckets=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)

    if [[ -z "$buckets" ]]; then
        log_info "No S3 buckets found"
        return 0
    fi

    for bucket in $buckets; do
        # Check if bucket is in the target region and matches project
        if matches_project "$bucket"; then
            # Get bucket location
            local bucket_region
            bucket_region=$(aws s3api get-bucket-location --bucket "$bucket" --query 'LocationConstraint' --output text 2>/dev/null || echo "us-east-1")

            # Handle null/None response for us-east-1
            [[ "$bucket_region" == "None" ]] || [[ "$bucket_region" == "null" ]] && bucket_region="us-east-1"

            if [[ "$bucket_region" == "$region" ]]; then
                log_info "Found replica bucket $bucket in $region"

                if confirm_destruction "Replica S3 Bucket" "$bucket (region: $region)"; then
                    log_action "Empty and delete replica S3 bucket: $bucket"

                    if [[ "$DRY_RUN" != "true" ]]; then
                        # Remove replication configuration if it exists
                        aws s3api delete-bucket-replication --bucket "$bucket" 2>/dev/null || true

                        # Empty bucket (all versions and delete markers)
                        aws s3api list-object-versions --bucket "$bucket" \
                            --query 'Versions[].{Key:Key,VersionId:VersionId}' \
                            --output text 2>/dev/null | \
                            while read -r key version_id; do
                                [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                            done

                        # Delete delete markers
                        aws s3api list-object-versions --bucket "$bucket" \
                            --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' \
                            --output text 2>/dev/null | \
                            while read -r key version_id; do
                                [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                            done

                        # Force empty using CLI
                        aws s3 rm "s3://$bucket" --recursive 2>/dev/null || true

                        # Delete bucket
                        if aws s3api delete-bucket --bucket "$bucket" --region "$region" 2>/dev/null; then
                            log_success "Deleted replica S3 bucket: $bucket"
                        else
                            log_error "Failed to delete replica S3 bucket: $bucket"
                        fi
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudFront distributions
destroy_cloudfront_distributions() {
    log_info "ðŸŒ Scanning for CloudFront distributions..."

    local distributions
    distributions=$(aws cloudfront list-distributions --query 'DistributionList.Items[].{Id:Id,DomainName:DomainName,Comment:Comment,Status:Status}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$distributions" == "null" ]] || [[ "$distributions" == "[]" ]] || [[ -z "$distributions" ]]; then
        log_info "No CloudFront distributions found"
        return 0
    fi

    echo "$distributions" | jq -c '.[]' | while read -r distribution; do
        local dist_id
        local comment
        local status

        dist_id=$(echo "$distribution" | jq -r '.Id')
        comment=$(echo "$distribution" | jq -r '.Comment // ""')
        status=$(echo "$distribution" | jq -r '.Status')

        if matches_project "$comment" || matches_project "$dist_id"; then
            if confirm_destruction "CloudFront Distribution" "$dist_id ($comment)"; then
                log_action "Disable and delete CloudFront distribution: $dist_id"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Get current config
                    local config etag
                    config=$(aws cloudfront get-distribution-config --id "$dist_id" --query 'DistributionConfig' --output json)
                    etag=$(aws cloudfront get-distribution-config --id "$dist_id" --query 'ETag' --output text)

                    # Disable distribution if enabled
                    if [[ "$(echo "$config" | jq -r '.Enabled')" == "true" ]]; then
                        config=$(echo "$config" | jq '.Enabled = false')

                        if aws cloudfront update-distribution --id "$dist_id" --distribution-config "$config" --if-match "$etag" >/dev/null; then
                            log_info "Distribution $dist_id disabled, waiting for deployment..."

                            # Wait for distribution to be deployed
                            local attempts=0
                            while [[ $attempts -lt 30 ]]; do
                                status=$(aws cloudfront get-distribution --id "$dist_id" --query 'Distribution.Status' --output text)
                                if [[ "$status" == "Deployed" ]]; then
                                    break
                                fi
                                log_info "Waiting for distribution $dist_id to be deployed (status: $status)..."
                                sleep 30
                                ((attempts++))
                            done
                        fi
                    fi

                    # Delete distribution (only works when disabled and deployed)
                    etag=$(aws cloudfront get-distribution --id "$dist_id" --query 'ETag' --output text)
                    if aws cloudfront delete-distribution --id "$dist_id" --if-match "$etag" 2>/dev/null; then
                        log_success "Deleted CloudFront distribution: $dist_id"
                    else
                        log_warn "Could not delete CloudFront distribution $dist_id (may need manual intervention)"
                    fi
                fi
            fi
        fi
    done
}

# Destroy DynamoDB tables
destroy_dynamodb_tables() {
    log_info "ðŸ—ƒï¸  Scanning for DynamoDB tables..."

    local tables
    tables=$(aws dynamodb list-tables --query 'TableNames[]' --output text 2>/dev/null || true)

    if [[ -z "$tables" ]]; then
        log_info "No DynamoDB tables found"
        return 0
    fi

    for table in $tables; do
        if matches_project "$table"; then
            if confirm_destruction "DynamoDB Table" "$table"; then
                log_action "Delete DynamoDB table: $table"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws dynamodb delete-table --table-name "$table" >/dev/null 2>&1; then
                        log_success "Deleted DynamoDB table: $table"
                    else
                        log_error "Failed to delete DynamoDB table: $table"
                    fi
                fi
            fi
        fi
    done
}

# Destroy KMS keys
destroy_kms_keys() {
    log_info "ðŸ” Scanning for KMS keys..."

    # Get all aliases first (with timeout to prevent hanging)
    local aliases
    aliases=$(timeout 10 aws kms list-aliases --query 'Aliases[].{AliasName:AliasName,TargetKeyId:TargetKeyId}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$aliases" == "null" ]] || [[ "$aliases" == "[]" ]] || [[ -z "$aliases" ]]; then
        log_info "No KMS aliases found"
        return 0
    fi

    echo "$aliases" | jq -c '.[]' | while read -r alias_info; do
        local alias_name target_key_id
        alias_name=$(echo "$alias_info" | jq -r '.AliasName')
        target_key_id=$(echo "$alias_info" | jq -r '.TargetKeyId // ""')

        if matches_project "$alias_name" && [[ -n "$target_key_id" ]]; then
            if confirm_destruction "KMS Key" "$alias_name ($target_key_id)"; then
                log_action "Schedule KMS key deletion: $alias_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Delete alias first
                    if aws kms delete-alias --alias-name "$alias_name" 2>/dev/null; then
                        log_success "Deleted KMS alias: $alias_name"
                    fi

                    # Schedule key deletion (7 day minimum)
                    if aws kms schedule-key-deletion --key-id "$target_key_id" --pending-window-in-days 7 >/dev/null 2>&1; then
                        log_success "Scheduled KMS key deletion: $target_key_id (7 days)"
                    else
                        log_error "Failed to schedule KMS key deletion: $target_key_id"
                    fi
                fi
            fi
        fi
    done
}

# Destroy cross-account GitHub Actions roles
destroy_cross_account_roles() {
    log_info "ðŸ” Scanning for cross-account GitHub Actions roles..."

    if [[ "$INCLUDE_CROSS_ACCOUNT" != "true" ]]; then
        log_info "Cross-account destruction disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Cross-account role destruction only supported from management account ($MANAGEMENT_ACCOUNT_ID)"
        log_warn "Current account: $current_account - skipping cross-account cleanup"
        return 0
    fi

    # Map account IDs to environment names
    local -A account_env_map=(
        ["822529998967"]="Dev"
        ["927588814642"]="Staging"
        ["546274483801"]="Prod"
    )

    for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
        if ! check_account_filter "$account_id"; then
            log_info "Skipping account $account_id - not in account filter"
            continue
        fi

        local env_name="${account_env_map[$account_id]}"
        local role_name="GitHubActions-StaticSite-${env_name}-Role"

        # Also check for alternative role naming patterns
        local alt_role_name="github-actions-workload-deployment"
        local org_role_arn="arn:aws:iam::${account_id}:role/OrganizationAccountAccessRole"

        log_info "Processing account $account_id ($env_name environment)"

        if confirm_destruction "Cross-Account Role" "$role_name in account $account_id"; then
            log_action "Destroy cross-account role: $role_name"

            if [[ "$DRY_RUN" != "true" ]]; then
                # Assume OrganizationAccountAccessRole in target account
                local assume_output
                if assume_output=$(aws sts assume-role \
                    --role-arn "$org_role_arn" \
                    --role-session-name "destroy-cross-account-${account_id}" \
                    --query 'Credentials.{AccessKeyId:AccessKeyId,SecretAccessKey:SecretAccessKey,SessionToken:SessionToken}' \
                    --output json 2>/dev/null); then

                    # Extract credentials
                    local access_key secret_key session_token
                    access_key=$(echo "$assume_output" | jq -r '.AccessKeyId')
                    secret_key=$(echo "$assume_output" | jq -r '.SecretAccessKey')
                    session_token=$(echo "$assume_output" | jq -r '.SessionToken')

                    # Check for both role naming patterns
                    local found_role=""
                    if AWS_ACCESS_KEY_ID="$access_key" \
                       AWS_SECRET_ACCESS_KEY="$secret_key" \
                       AWS_SESSION_TOKEN="$session_token" \
                       aws iam get-role --role-name "$role_name" >/dev/null 2>&1; then
                        found_role="$role_name"
                    elif AWS_ACCESS_KEY_ID="$access_key" \
                         AWS_SECRET_ACCESS_KEY="$secret_key" \
                         AWS_SESSION_TOKEN="$session_token" \
                         aws iam get-role --role-name "$alt_role_name" >/dev/null 2>&1; then
                        found_role="$alt_role_name"
                    fi

                    if [[ -n "$found_role" ]]; then
                        log_info "Found role $found_role in account $account_id - proceeding with destruction"

                        # Detach managed policies
                        AWS_ACCESS_KEY_ID="$access_key" \
                        AWS_SECRET_ACCESS_KEY="$secret_key" \
                        AWS_SESSION_TOKEN="$session_token" \
                        aws iam list-attached-role-policies --role-name "$found_role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                            while read -r policy_arn; do
                                [[ -n "$policy_arn" ]] && AWS_ACCESS_KEY_ID="$access_key" \
                                AWS_SECRET_ACCESS_KEY="$secret_key" \
                                AWS_SESSION_TOKEN="$session_token" \
                                aws iam detach-role-policy --role-name "$found_role" --policy-arn "$policy_arn" 2>/dev/null || true
                            done

                        # Delete inline policies
                        AWS_ACCESS_KEY_ID="$access_key" \
                        AWS_SECRET_ACCESS_KEY="$secret_key" \
                        AWS_SESSION_TOKEN="$session_token" \
                        aws iam list-role-policies --role-name "$found_role" --query 'PolicyNames[]' --output text 2>/dev/null | \
                            while read -r policy_name; do
                                [[ -n "$policy_name" ]] && AWS_ACCESS_KEY_ID="$access_key" \
                                AWS_SECRET_ACCESS_KEY="$secret_key" \
                                AWS_SESSION_TOKEN="$session_token" \
                                aws iam delete-role-policy --role-name "$found_role" --policy-name "$policy_name" 2>/dev/null || true
                            done

                        # Delete the role
                        if AWS_ACCESS_KEY_ID="$access_key" \
                           AWS_SECRET_ACCESS_KEY="$secret_key" \
                           AWS_SESSION_TOKEN="$session_token" \
                           aws iam delete-role --role-name "$found_role" 2>/dev/null; then
                            log_success "Deleted cross-account role: $found_role in account $account_id"
                        else
                            log_error "Failed to delete cross-account role: $found_role in account $account_id"
                        fi
                    else
                        log_info "Role $role_name not found in account $account_id - skipping"
                    fi
                else
                    log_error "Failed to assume OrganizationAccountAccessRole in account $account_id"
                    log_error "Ensure the role exists and this account has permission to assume it"
                fi
            fi
        fi
    done
}

# Destroy IAM resources
destroy_iam_resources() {
    log_info "ðŸ‘¤ Scanning for IAM resources..."

    # Get current account ID for validation
    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    if ! check_account_filter "$current_account"; then
        log_warn "Skipping IAM resources - account $current_account not in filter"
        return 0
    fi

    # Destroy IAM roles (with timeout to prevent hanging)
    local roles
    roles=$(timeout 15 aws iam list-roles --query 'Roles[].{RoleName:RoleName,Arn:Arn}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$roles" == "null" ]] || [[ "$roles" == "[]" ]] || [[ -z "$roles" ]]; then
        log_info "No IAM roles found"
    else
        echo "$roles" | jq -c '.[]' | while read -r role_info; do
        local role_name role_arn
        role_name=$(echo "$role_info" | jq -r '.RoleName')
        role_arn=$(echo "$role_info" | jq -r '.Arn')

        if matches_project "$role_name"; then
            if confirm_destruction "IAM Role" "$role_name"; then
                log_action "Delete IAM role: $role_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Detach managed policies
                    aws iam list-attached-role-policies --role-name "$role_name" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                        while read -r policy_arn; do
                            [[ -n "$policy_arn" ]] && aws iam detach-role-policy --role-name "$role_name" --policy-arn "$policy_arn" 2>/dev/null || true
                        done

                    # Delete inline policies
                    aws iam list-role-policies --role-name "$role_name" --query 'PolicyNames[]' --output text 2>/dev/null | \
                        while read -r policy_name; do
                            [[ -n "$policy_name" ]] && aws iam delete-role-policy --role-name "$role_name" --policy-name "$policy_name" 2>/dev/null || true
                        done

                    # Delete role
                    if aws iam delete-role --role-name "$role_name" 2>/dev/null; then
                        log_success "Deleted IAM role: $role_name"
                    else
                        log_error "Failed to delete IAM role: $role_name"
                    fi
                fi
            fi
        fi
        done
    fi

    # Destroy custom IAM policies (with timeout)
    local policies
    policies=$(timeout 15 aws iam list-policies --scope Local --query 'Policies[].{PolicyName:PolicyName,Arn:Arn}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$policies" == "null" ]] || [[ "$policies" == "[]" ]] || [[ -z "$policies" ]]; then
        log_info "No custom IAM policies found"
    else
        echo "$policies" | jq -c '.[]' | while read -r policy_info; do
        local policy_name policy_arn
        policy_name=$(echo "$policy_info" | jq -r '.PolicyName')
        policy_arn=$(echo "$policy_info" | jq -r '.Arn')

        if matches_project "$policy_name"; then
            if confirm_destruction "IAM Policy" "$policy_name"; then
                log_action "Delete IAM policy: $policy_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Delete all policy versions except default
                    aws iam list-policy-versions --policy-arn "$policy_arn" --query 'Versions[?!IsDefaultVersion].VersionId' --output text 2>/dev/null | \
                        while read -r version_id; do
                            [[ -n "$version_id" ]] && aws iam delete-policy-version --policy-arn "$policy_arn" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Delete policy
                    if aws iam delete-policy --policy-arn "$policy_arn" 2>/dev/null; then
                        log_success "Deleted IAM policy: $policy_name"
                    else
                        log_error "Failed to delete IAM policy: $policy_name"
                    fi
                fi
            fi
        fi
        done
    fi

    # Destroy OIDC identity providers
    local oidc_providers
    oidc_providers=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[].Arn' --output text 2>/dev/null || true)

    for provider_arn in $oidc_providers; do
        if [[ "$provider_arn" == *"token.actions.githubusercontent.com"* ]]; then
            if confirm_destruction "OIDC Identity Provider" "$provider_arn"; then
                log_action "Delete OIDC identity provider: $provider_arn"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws iam delete-open-id-connect-provider --open-id-connect-provider-arn "$provider_arn" 2>/dev/null; then
                        log_success "Deleted OIDC identity provider: $provider_arn"
                    else
                        log_error "Failed to delete OIDC identity provider: $provider_arn"
                    fi
                fi
            fi
        fi
    done

    # Destroy IAM users
    log_info "ðŸ‘¤ Scanning for IAM users..."
    local users
    users=$(aws iam list-users --query 'Users[].UserName' --output text 2>/dev/null || true)

    for user_name in $users; do
        if matches_project "$user_name"; then
            if confirm_destruction "IAM User" "$user_name"; then
                log_action "Delete IAM user: $user_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Remove user from all groups
                    aws iam list-groups-for-user --user-name "$user_name" --query 'Groups[].GroupName' --output text 2>/dev/null | \
                        while read -r group_name; do
                            [[ -n "$group_name" ]] && aws iam remove-user-from-group --user-name "$user_name" --group-name "$group_name" 2>/dev/null || true
                        done

                    # Delete access keys
                    aws iam list-access-keys --user-name "$user_name" --query 'AccessKeyMetadata[].AccessKeyId' --output text 2>/dev/null | \
                        while read -r access_key_id; do
                            [[ -n "$access_key_id" ]] && aws iam delete-access-key --user-name "$user_name" --access-key-id "$access_key_id" 2>/dev/null || true
                        done

                    # Delete MFA devices
                    aws iam list-mfa-devices --user-name "$user_name" --query 'MFADevices[].SerialNumber' --output text 2>/dev/null | \
                        while read -r serial_number; do
                            [[ -n "$serial_number" ]] && aws iam deactivate-mfa-device --user-name "$user_name" --serial-number "$serial_number" 2>/dev/null || true
                            [[ -n "$serial_number" ]] && aws iam delete-virtual-mfa-device --serial-number "$serial_number" 2>/dev/null || true
                        done

                    # Delete signing certificates
                    aws iam list-signing-certificates --user-name "$user_name" --query 'Certificates[].CertificateId' --output text 2>/dev/null | \
                        while read -r cert_id; do
                            [[ -n "$cert_id" ]] && aws iam delete-signing-certificate --user-name "$user_name" --certificate-id "$cert_id" 2>/dev/null || true
                        done

                    # Delete SSH public keys
                    aws iam list-ssh-public-keys --user-name "$user_name" --query 'SSHPublicKeys[].SSHPublicKeyId' --output text 2>/dev/null | \
                        while read -r ssh_key_id; do
                            [[ -n "$ssh_key_id" ]] && aws iam delete-ssh-public-key --user-name "$user_name" --ssh-public-key-id "$ssh_key_id" 2>/dev/null || true
                        done

                    # Delete service specific credentials
                    aws iam list-service-specific-credentials --user-name "$user_name" --query 'ServiceSpecificCredentials[].ServiceSpecificCredentialId' --output text 2>/dev/null | \
                        while read -r cred_id; do
                            [[ -n "$cred_id" ]] && aws iam delete-service-specific-credential --user-name "$user_name" --service-specific-credential-id "$cred_id" 2>/dev/null || true
                        done

                    # Detach managed policies
                    aws iam list-attached-user-policies --user-name "$user_name" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                        while read -r policy_arn; do
                            [[ -n "$policy_arn" ]] && aws iam detach-user-policy --user-name "$user_name" --policy-arn "$policy_arn" 2>/dev/null || true
                        done

                    # Delete inline policies
                    aws iam list-user-policies --user-name "$user_name" --query 'PolicyNames[]' --output text 2>/dev/null | \
                        while read -r policy_name; do
                            [[ -n "$policy_name" ]] && aws iam delete-user-policy --user-name "$user_name" --policy-name "$policy_name" 2>/dev/null || true
                        done

                    # Delete login profile (console access)
                    aws iam delete-login-profile --user-name "$user_name" 2>/dev/null || true

                    # Delete user
                    if aws iam delete-user --user-name "$user_name" 2>/dev/null; then
                        log_success "Deleted IAM user: $user_name"
                    else
                        log_error "Failed to delete IAM user: $user_name"
                    fi
                fi
            fi
        fi
    done

    # Destroy IAM groups
    log_info "ðŸ‘¥ Scanning for IAM groups..."
    local groups
    groups=$(aws iam list-groups --query 'Groups[].GroupName' --output text 2>/dev/null || true)

    for group_name in $groups; do
        if matches_project "$group_name"; then
            if confirm_destruction "IAM Group" "$group_name"; then
                log_action "Delete IAM group: $group_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Remove all users from group
                    aws iam get-group --group-name "$group_name" --query 'Users[].UserName' --output text 2>/dev/null | \
                        while read -r user_name; do
                            [[ -n "$user_name" ]] && aws iam remove-user-from-group --user-name "$user_name" --group-name "$group_name" 2>/dev/null || true
                        done

                    # Detach managed policies
                    aws iam list-attached-group-policies --group-name "$group_name" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                        while read -r policy_arn; do
                            [[ -n "$policy_arn" ]] && aws iam detach-group-policy --group-name "$group_name" --policy-arn "$policy_arn" 2>/dev/null || true
                        done

                    # Delete inline policies
                    aws iam list-group-policies --group-name "$group_name" --query 'PolicyNames[]' --output text 2>/dev/null | \
                        while read -r policy_name; do
                            [[ -n "$policy_name" ]] && aws iam delete-group-policy --group-name "$group_name" --policy-name "$policy_name" 2>/dev/null || true
                        done

                    # Delete group
                    if aws iam delete-group --group-name "$group_name" 2>/dev/null; then
                        log_success "Deleted IAM group: $group_name"
                    else
                        log_error "Failed to delete IAM group: $group_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudWatch resources
destroy_cloudwatch_resources() {
    log_info "ðŸ“Š Scanning for CloudWatch resources..."

    # Destroy log groups
    local log_groups
    log_groups=$(aws logs describe-log-groups --query 'logGroups[].logGroupName' --output text 2>/dev/null || true)

    for log_group in $log_groups; do
        if matches_project "$log_group" || [[ "$log_group" == *"/aws/cloudtrail"* ]]; then
            if confirm_destruction "CloudWatch Log Group" "$log_group"; then
                log_action "Delete CloudWatch log group: $log_group"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws logs delete-log-group --log-group-name "$log_group" 2>/dev/null; then
                        log_success "Deleted CloudWatch log group: $log_group"
                    else
                        log_error "Failed to delete CloudWatch log group: $log_group"
                    fi
                fi
            fi
        fi
    done

    # Destroy alarms
    local alarms
    alarms=$(aws cloudwatch describe-alarms --query 'MetricAlarms[].AlarmName' --output text 2>/dev/null || true)

    for alarm in $alarms; do
        if matches_project "$alarm"; then
            if confirm_destruction "CloudWatch Alarm" "$alarm"; then
                log_action "Delete CloudWatch alarm: $alarm"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws cloudwatch delete-alarms --alarm-names "$alarm" 2>/dev/null; then
                        log_success "Deleted CloudWatch alarm: $alarm"
                    else
                        log_error "Failed to delete CloudWatch alarm: $alarm"
                    fi
                fi
            fi
        fi
    done
}

# Destroy SNS resources
destroy_sns_resources() {
    log_info "ðŸ“¢ Scanning for SNS resources..."

    local topics
    topics=$(aws sns list-topics --query 'Topics[].TopicArn' --output text 2>/dev/null || true)

    for topic_arn in $topics; do
        local topic_name
        topic_name=$(basename "$topic_arn")

        if matches_project "$topic_name"; then
            if confirm_destruction "SNS Topic" "$topic_name"; then
                log_action "Delete SNS topic: $topic_arn"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws sns delete-topic --topic-arn "$topic_arn" 2>/dev/null; then
                        log_success "Deleted SNS topic: $topic_name"
                    else
                        log_error "Failed to delete SNS topic: $topic_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy Route53 resources
destroy_route53_resources() {
    log_info "ðŸŒ Scanning for Route53 resources..."

    # Destroy health checks
    local health_checks
    health_checks=$(aws route53 list-health-checks --query 'HealthChecks[].Id' --output text 2>/dev/null || true)

    for health_check_id in $health_checks; do
        local health_check_config
        health_check_config=$(aws route53 get-health-check --health-check-id "$health_check_id" --query 'HealthCheck.HealthCheckConfig.FullyQualifiedDomainName' --output text 2>/dev/null || echo "unknown")

        if matches_project "$health_check_config" || [[ "$health_check_config" == *"static-site"* ]]; then
            if confirm_destruction "Route53 Health Check" "$health_check_id ($health_check_config)"; then
                log_action "Delete Route53 health check: $health_check_id"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws route53 delete-health-check --health-check-id "$health_check_id" 2>/dev/null; then
                        log_success "Deleted Route53 health check: $health_check_id"
                    else
                        log_error "Failed to delete Route53 health check: $health_check_id"
                    fi
                fi
            fi
        fi
    done

    # Destroy hosted zones
    local hosted_zones
    hosted_zones=$(aws route53 list-hosted-zones --query 'HostedZones[].{Id:Id,Name:Name}' --output json 2>/dev/null || echo "[]")

    if [[ "$hosted_zones" != "[]" ]] && [[ "$hosted_zones" != "null" ]] && [[ -n "$hosted_zones" ]]; then
        echo "$hosted_zones" | jq -c '.[]' | while read -r zone; do
            local zone_id zone_name
            zone_id=$(echo "$zone" | jq -r '.Id' | cut -d'/' -f3)
            zone_name=$(echo "$zone" | jq -r '.Name')

            if matches_project "$zone_name"; then
                if confirm_destruction "Route53 Hosted Zone" "$zone_name ($zone_id)"; then
                    log_action "Delete Route53 hosted zone: $zone_name"

                    if [[ "$DRY_RUN" != "true" ]]; then
                        # Delete all non-essential records first (keep SOA and NS for the zone itself)
                        local records
                        records=$(aws route53 list-resource-record-sets --hosted-zone-id "$zone_id" --query 'ResourceRecordSets[?Type!=`SOA` && Type!=`NS`].{Name:Name,Type:Type}' --output json 2>/dev/null || echo "[]")

                        echo "$records" | jq -c '.[]' | while read -r record; do
                            local record_name record_type
                            record_name=$(echo "$record" | jq -r '.Name')
                            record_type=$(echo "$record" | jq -r '.Type')

                            log_info "Deleting record: $record_name ($record_type)"

                            # Get full record details for deletion
                            local record_data
                            record_data=$(aws route53 list-resource-record-sets --hosted-zone-id "$zone_id" --query "ResourceRecordSets[?Name=='$record_name' && Type=='$record_type']" --output json 2>/dev/null || echo "[]")

                            if [[ "$record_data" != "[]" ]]; then
                                # Create change batch for deletion
                                local change_batch
                                change_batch=$(echo "$record_data" | jq '{Changes: [{Action: "DELETE", ResourceRecordSet: .[0]}]}')

                                aws route53 change-resource-record-sets --hosted-zone-id "$zone_id" --change-batch "$change_batch" 2>/dev/null || log_warn "Failed to delete record $record_name"
                            fi
                        done

                        # Now delete the hosted zone
                        if aws route53 delete-hosted-zone --id "$zone_id" 2>/dev/null; then
                            log_success "Deleted Route53 hosted zone: $zone_name"
                        else
                            log_error "Failed to delete Route53 hosted zone: $zone_name (may still have records)"
                        fi
                    fi
                fi
            fi
        done
    fi
}

# Destroy CloudWatch dashboards and composite alarms
destroy_cloudwatch_dashboards() {
    log_info "ðŸ“Š Scanning for CloudWatch dashboards..."

    local dashboards
    dashboards=$(aws cloudwatch list-dashboards --query 'DashboardEntries[].DashboardName' --output text 2>/dev/null || true)

    for dashboard_name in $dashboards; do
        if matches_project "$dashboard_name"; then
            if confirm_destruction "CloudWatch Dashboard" "$dashboard_name"; then
                log_action "Delete CloudWatch dashboard: $dashboard_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws cloudwatch delete-dashboards --dashboard-names "$dashboard_name" 2>/dev/null; then
                        log_success "Deleted CloudWatch dashboard: $dashboard_name"
                    else
                        log_error "Failed to delete CloudWatch dashboard: $dashboard_name"
                    fi
                fi
            fi
        fi
    done

    # Destroy composite alarms
    log_info "ðŸ“Š Scanning for CloudWatch composite alarms..."
    local composite_alarms
    composite_alarms=$(aws cloudwatch describe-alarms --alarm-types CompositeAlarm --query 'CompositeAlarms[].AlarmName' --output text 2>/dev/null || true)

    for alarm_name in $composite_alarms; do
        if matches_project "$alarm_name"; then
            if confirm_destruction "CloudWatch Composite Alarm" "$alarm_name"; then
                log_action "Delete CloudWatch composite alarm: $alarm_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws cloudwatch delete-alarms --alarm-names "$alarm_name" 2>/dev/null; then
                        log_success "Deleted CloudWatch composite alarm: $alarm_name"
                    else
                        log_error "Failed to delete CloudWatch composite alarm: $alarm_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy AWS Budgets
destroy_aws_budgets() {
    log_info "ðŸ’° Scanning for AWS Budgets..."

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    local budgets
    budgets=$(aws budgets describe-budgets --account-id "$current_account" --query 'Budgets[].BudgetName' --output text 2>/dev/null || true)

    for budget_name in $budgets; do
        if matches_project "$budget_name" || [[ "$budget_name" == *"static-site"* ]]; then
            if confirm_destruction "AWS Budget" "$budget_name"; then
                log_action "Delete AWS Budget: $budget_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Delete budget actions first
                    local actions
                    actions=$(aws budgets describe-budget-actions-for-budget --account-id "$current_account" --budget-name "$budget_name" --query 'Actions[].ActionId' --output text 2>/dev/null || true)

                    for action_id in $actions; do
                        log_info "Deleting budget action: $action_id"
                        aws budgets delete-budget-action --account-id "$current_account" --budget-name "$budget_name" --action-id "$action_id" 2>/dev/null || true
                    done

                    # Delete the budget
                    if aws budgets delete-budget --account-id "$current_account" --budget-name "$budget_name" 2>/dev/null; then
                        log_success "Deleted AWS Budget: $budget_name"
                    else
                        log_error "Failed to delete AWS Budget: $budget_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy SSM Parameters
destroy_ssm_parameters() {
    log_info "ðŸ”§ Scanning for SSM Parameters..."

    local parameters
    parameters=$(aws ssm describe-parameters --query 'Parameters[].Name' --output text 2>/dev/null || true)

    for param_name in $parameters; do
        if matches_project "$param_name"; then
            if confirm_destruction "SSM Parameter" "$param_name"; then
                log_action "Delete SSM parameter: $param_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws ssm delete-parameter --name "$param_name" 2>/dev/null; then
                        log_success "Deleted SSM parameter: $param_name"
                    else
                        log_error "Failed to delete SSM parameter: $param_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudTrail resources
destroy_cloudtrail_resources() {
    log_info "ðŸ“‹ Scanning for CloudTrail resources..."

    local trails
    trails=$(aws cloudtrail describe-trails --query 'trailList[].{Name:Name,S3BucketName:S3BucketName}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$trails" == "null" ]] || [[ "$trails" == "[]" ]] || [[ -z "$trails" ]]; then
        log_info "No CloudTrail trails found"
        return 0
    fi

    echo "$trails" | jq -c '.[]' | while read -r trail_info; do
        local trail_name s3_bucket
        trail_name=$(echo "$trail_info" | jq -r '.Name')
        s3_bucket=$(echo "$trail_info" | jq -r '.S3BucketName // ""')

        if matches_project "$trail_name" || matches_project "$s3_bucket"; then
            if confirm_destruction "CloudTrail Trail" "$trail_name"; then
                log_action "Delete CloudTrail trail: $trail_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Stop logging first
                    aws cloudtrail stop-logging --name "$trail_name" 2>/dev/null || true

                    # Delete trail
                    if aws cloudtrail delete-trail --name "$trail_name" 2>/dev/null; then
                        log_success "Deleted CloudTrail trail: $trail_name"
                    else
                        log_error "Failed to delete CloudTrail trail: $trail_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy WAF resources
destroy_waf_resources() {
    log_info "ðŸ›¡ï¸  Scanning for WAF resources..."

    # Check CloudFront scope
    local web_acls
    web_acls=$(aws wafv2 list-web-acls --scope CLOUDFRONT --query 'WebACLs[].{Name:Name,Id:Id}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$web_acls" == "null" ]] || [[ "$web_acls" == "[]" ]] || [[ -z "$web_acls" ]]; then
        log_info "No WAF Web ACLs found"
        return 0
    fi

    echo "$web_acls" | jq -c '.[]' | while read -r web_acl; do
        local name id
        name=$(echo "$web_acl" | jq -r '.Name')
        id=$(echo "$web_acl" | jq -r '.Id')

        if matches_project "$name"; then
            if confirm_destruction "WAF Web ACL" "$name"; then
                log_action "Delete WAF Web ACL: $name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Get lock token
                    local lock_token
                    lock_token=$(aws wafv2 get-web-acl --scope CLOUDFRONT --id "$id" --name "$name" --query 'LockToken' --output text 2>/dev/null || true)

                    if [[ -n "$lock_token" ]] && aws wafv2 delete-web-acl --scope CLOUDFRONT --id "$id" --name "$name" --lock-token "$lock_token" 2>/dev/null; then
                        log_success "Deleted WAF Web ACL: $name"
                    else
                        log_error "Failed to delete WAF Web ACL: $name"
                    fi
                fi
            fi
        fi
    done
}

# Cleanup Terraform state for cross-account modules
cleanup_terraform_state() {
    log_info "ðŸ—‚ï¸  Cleaning up Terraform state for cross-account modules..."

    if [[ "$CLEANUP_TERRAFORM_STATE" != "true" ]]; then
        log_info "Terraform state cleanup disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Terraform state cleanup only supported from management account"
        return 0
    fi

    # Check if we're in the terraform directory
    local terraform_dir="$SCRIPT_DIR/../terraform/foundations/org-management"
    if [[ ! -d "$terraform_dir" ]]; then
        log_warn "Terraform directory not found: $terraform_dir"
        return 0
    fi

    if confirm_destruction "Terraform State" "cross-account-roles module state"; then
        log_action "Clean Terraform state for cross-account-roles module"

        if [[ "$DRY_RUN" != "true" ]]; then
            pushd "$terraform_dir" >/dev/null 2>&1 || {
                log_error "Failed to change to terraform directory: $terraform_dir"
                return 1
            }

            # Initialize terraform if needed
            if [[ ! -d ".terraform" ]]; then
                log_info "Initializing Terraform..."
                if ! tofu init -upgrade; then
                    log_error "Failed to initialize Terraform"
                    popd >/dev/null 2>&1 || true
                    return 1
                fi
            fi

            # List state resources related to cross-account roles
            log_info "Checking for cross-account role resources in state..."
            local state_resources
            state_resources=$(tofu state list 2>/dev/null | grep -E "(cross_account|cross-account)" || true)

            if [[ -n "$state_resources" ]]; then
                log_info "Found cross-account resources in state:"
                echo "$state_resources" | while read -r resource; do
                    log_info "  - $resource"
                done

                # Remove cross-account resources from state
                echo "$state_resources" | while read -r resource; do
                    if [[ -n "$resource" ]]; then
                        log_action "Remove from state: $resource"
                        if tofu state rm "$resource" 2>/dev/null; then
                            log_success "Removed from state: $resource"
                        else
                            log_warn "Failed to remove from state: $resource"
                        fi
                    fi
                done
            else
                log_info "No cross-account resources found in Terraform state"
            fi

            # Also check for any orphaned modules
            local module_resources
            module_resources=$(tofu state list 2>/dev/null | grep -E "module\.(cross_account|cross-account)" || true)

            if [[ -n "$module_resources" ]]; then
                log_info "Found cross-account module resources in state:"
                echo "$module_resources" | while read -r resource; do
                    log_info "  - $resource"
                    log_action "Remove module from state: $resource"
                    if tofu state rm "$resource" 2>/dev/null; then
                        log_success "Removed module from state: $resource"
                    else
                        log_warn "Failed to remove module from state: $resource"
                    fi
                done
            fi

            popd >/dev/null 2>&1 || true
            log_success "Terraform state cleanup completed"
        fi
    fi
}

# Close member accounts (optional)
close_member_accounts() {
    log_info "ðŸ¢ Processing member account closure..."

    if [[ "$CLOSE_MEMBER_ACCOUNTS" != "true" ]]; then
        log_info "Member account closure disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Account closure only supported from management account"
        return 0
    fi

    # Verify we have organization access
    if ! aws organizations describe-organization >/dev/null 2>&1; then
        log_error "Unable to access AWS Organizations - cannot close member accounts"
        return 1
    fi

    log_warn "âš ï¸  ACCOUNT CLOSURE LIMITATIONS:"
    log_warn "   - Can only close 10% of member accounts within rolling 30-day period"
    log_warn "   - Closed accounts remain in organization for 90 days"
    log_warn "   - Outstanding fees and Reserved Instance charges still apply"
    log_warn "   - AWS Marketplace subscriptions must be manually canceled"

    # Map account IDs to environment names
    local -A account_env_map=(
        ["822529998967"]="Dev"
        ["927588814642"]="Staging"
        ["546274483801"]="Prod"
    )

    for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
        if ! check_account_filter "$account_id"; then
            log_info "Skipping account closure for $account_id - not in account filter"
            continue
        fi

        local env_name="${account_env_map[$account_id]}"

        # Check account status first
        local account_status
        account_status=$(aws organizations list-accounts --query "Accounts[?Id=='$account_id'].Status" --output text 2>/dev/null || echo "UNKNOWN")

        if [[ "$account_status" == "CLOSED" ]]; then
            log_info "Account $account_id ($env_name) is already closed - skipping"
            continue
        elif [[ "$account_status" == "UNKNOWN" ]]; then
            log_warn "Unable to determine status of account $account_id ($env_name) - skipping"
            continue
        fi

        log_warn "âš ï¸  About to close member account: $account_id ($env_name)"
        log_warn "   This action cannot be undone for 90 days"
        log_warn "   Ensure all critical resources have been backed up"

        if confirm_destruction "Member Account" "$account_id ($env_name) - PERMANENT CLOSURE"; then
            log_action "Close member account: $account_id ($env_name)"

            if [[ "$DRY_RUN" != "true" ]]; then
                # Note: AWS CLI doesn't currently support member account closure
                # This would need to be done via AWS Console or Organizations API
                log_warn "Member account closure must be performed manually via AWS Console"
                log_warn "Navigate to: AWS Organizations > Accounts > $account_id > Close account"
                log_warn "Enter account ID '$account_id' to confirm closure"

                # Future enhancement: Implement via AWS Organizations API when available
                # if aws organizations close-account --account-id "$account_id" 2>/dev/null; then
                #     log_success "Initiated closure of member account: $account_id ($env_name)"
                # else
                #     log_error "Failed to close member account: $account_id ($env_name)"
                # fi
            fi
        fi
    done

    if [[ "$DRY_RUN" != "true" ]]; then
        log_info "After manual account closure:"
        log_info "  - Accounts will show 'CLOSED' status for up to 90 days"
        log_info "  - Final bills will be generated for services used before closure"
        log_info "  - Reserved Instance charges will continue until expiration"
        log_info "  - You can reopen accounts during the 90-day period if needed"
    fi
}

# Destroy AWS Organizations resources
destroy_organizations_resources() {
    log_info "ðŸ¢ Scanning for AWS Organizations resources..."

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "AWS Organizations cleanup only supported from management account"
        return 0
    fi

    # Check if organization exists
    if ! aws organizations describe-organization >/dev/null 2>&1; then
        log_info "No AWS Organization found - skipping"
        return 0
    fi

    log_info "Processing AWS Organizations structure..."

    # Step 1: Detach SCPs from OUs and accounts
    log_info "Detaching Service Control Policies..."
    local policies
    policies=$(aws organizations list-policies --filter SERVICE_CONTROL_POLICY --query 'Policies[?Name!=`FullAWSAccess`].Id' --output text 2>/dev/null || true)

    for policy_id in $policies; do
        local policy_name
        policy_name=$(aws organizations describe-policy --policy-id "$policy_id" --query 'Policy.PolicySummary.Name' --output text 2>/dev/null || echo "unknown")

        # Get all targets for this policy
        local targets
        targets=$(aws organizations list-targets-for-policy --policy-id "$policy_id" --query 'Targets[].TargetId' --output text 2>/dev/null || true)

        for target_id in $targets; do
            if confirm_destruction "SCP Attachment" "$policy_name from $target_id"; then
                log_action "Detach SCP $policy_name from $target_id"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws organizations detach-policy --policy-id "$policy_id" --target-id "$target_id" 2>/dev/null; then
                        log_success "Detached SCP $policy_name from $target_id"
                    else
                        log_error "Failed to detach SCP $policy_name from $target_id"
                    fi
                fi
            fi
        done
    done

    # Step 2: Delete custom SCPs
    log_info "Deleting custom Service Control Policies..."
    for policy_id in $policies; do
        local policy_name
        policy_name=$(aws organizations describe-policy --policy-id "$policy_id" --query 'Policy.PolicySummary.Name' --output text 2>/dev/null || echo "unknown")

        if confirm_destruction "Service Control Policy" "$policy_name"; then
            log_action "Delete SCP: $policy_name"

            if [[ "$DRY_RUN" != "true" ]]; then
                if aws organizations delete-policy --policy-id "$policy_id" 2>/dev/null; then
                    log_success "Deleted SCP: $policy_name"
                else
                    log_error "Failed to delete SCP: $policy_name"
                fi
            fi
        fi
    done

    # Step 3: Move accounts from OUs back to root (if closing accounts)
    if [[ "$CLOSE_MEMBER_ACCOUNTS" == "true" ]]; then
        log_info "Moving member accounts to root OU..."
        local root_id
        root_id=$(aws organizations list-roots --query 'Roots[0].Id' --output text 2>/dev/null)

        for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
            if ! check_account_filter "$account_id"; then
                continue
            fi

            # Find current parent OU
            local parent_id
            parent_id=$(aws organizations list-parents --child-id "$account_id" --query 'Parents[0].Id' --output text 2>/dev/null || true)

            if [[ -n "$parent_id" ]] && [[ "$parent_id" != "$root_id" ]]; then
                log_action "Move account $account_id to root OU"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws organizations move-account --account-id "$account_id" --source-parent-id "$parent_id" --destination-parent-id "$root_id" 2>/dev/null; then
                        log_success "Moved account $account_id to root"
                    else
                        log_error "Failed to move account $account_id"
                    fi
                fi
            fi
        done
    fi

    # Step 4: Delete OUs (bottom-up, children first)
    log_info "Deleting Organizational Units..."
    local root_id
    root_id=$(aws organizations list-roots --query 'Roots[0].Id' --output text 2>/dev/null)

    # Function to recursively delete OUs
    delete_ous_recursive() {
        local parent_id="$1"

        # List all child OUs
        local child_ous
        child_ous=$(aws organizations list-organizational-units-for-parent --parent-id "$parent_id" --query 'OrganizationalUnits[].Id' --output text 2>/dev/null || true)

        for ou_id in $child_ous; do
            # Recursively delete children first
            delete_ous_recursive "$ou_id"

            # Now delete this OU
            local ou_name
            ou_name=$(aws organizations describe-organizational-unit --organizational-unit-id "$ou_id" --query 'OrganizationalUnit.Name' --output text 2>/dev/null || echo "unknown")

            if confirm_destruction "Organizational Unit" "$ou_name ($ou_id)"; then
                log_action "Delete OU: $ou_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Move any accounts in this OU to root first
                    local accounts_in_ou
                    accounts_in_ou=$(aws organizations list-accounts-for-parent --parent-id "$ou_id" --query 'Accounts[].Id' --output text 2>/dev/null || true)

                    for account_id in $accounts_in_ou; do
                        log_info "Moving account $account_id from OU $ou_name to root"
                        aws organizations move-account --account-id "$account_id" --source-parent-id "$ou_id" --destination-parent-id "$root_id" 2>/dev/null || true
                    done

                    # Delete the OU
                    if aws organizations delete-organizational-unit --organizational-unit-id "$ou_id" 2>/dev/null; then
                        log_success "Deleted OU: $ou_name"
                    else
                        log_error "Failed to delete OU: $ou_name (may have accounts or child OUs)"
                    fi
                fi
            fi
        done
    }

    # Start deletion from root
    if [[ -n "$root_id" ]]; then
        delete_ous_recursive "$root_id"
    fi

    log_info "AWS Organizations cleanup completed"
}

# Cleanup orphaned resources that cost money
cleanup_orphaned_resources() {
    log_info "ðŸ§¹ Scanning for orphaned resources that cost money..."

    # Unassociated Elastic IPs
    log_info "Checking for unassociated Elastic IPs..."
    local eips
    eips=$(timeout 30 aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].{PublicIp:PublicIp,AllocationId:AllocationId}' --output json 2>/dev/null || echo "[]")

    if [[ "$eips" != "[]" ]] && [[ "$eips" != "null" ]] && [[ -n "$eips" ]]; then
        echo "$eips" | jq -c '.[]' | while read -r eip; do
            local public_ip allocation_id
            public_ip=$(echo "$eip" | jq -r '.PublicIp')
            allocation_id=$(echo "$eip" | jq -r '.AllocationId')

            if confirm_destruction "Orphaned Elastic IP" "$public_ip"; then
                log_action "Release Elastic IP: $public_ip"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws ec2 release-address --allocation-id "$allocation_id" 2>/dev/null; then
                        log_success "Released Elastic IP: $public_ip"
                    else
                        log_error "Failed to release Elastic IP: $public_ip"
                    fi
                fi
            fi
        done
    fi
}

# Generate cost estimate
generate_cost_estimate() {
    log_info "ðŸ’° Generating monthly cost estimate for destroyed resources..."

    # This is a rough estimate based on typical AWS pricing
    local total_monthly_savings=0

    # Estimate savings (very rough)
    local s3_buckets_count
    s3_buckets_count=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null | wc -w 2>/dev/null || echo 0)
    s3_buckets_count=$(echo "$s3_buckets_count" | tr -d '[:space:]')
    [[ ! "$s3_buckets_count" =~ ^[0-9]+$ ]] && s3_buckets_count=0

    local cloudfront_count
    cloudfront_count=$(aws cloudfront list-distributions --query 'DistributionList.Items[].Id' --output text 2>/dev/null | wc -w 2>/dev/null || echo 0)
    cloudfront_count=$(echo "$cloudfront_count" | tr -d '[:space:]')
    [[ ! "$cloudfront_count" =~ ^[0-9]+$ ]] && cloudfront_count=0

    # Rough monthly cost estimates (in USD)
    local s3_cost=$((s3_buckets_count * 5))      # ~$5/month per bucket (very rough)
    local cloudfront_cost=$((cloudfront_count * 10))  # ~$10/month per distribution
    local dynamodb_cost=5                        # ~$5/month for state locking
    local kms_cost=10                            # ~$1/month per key + usage

    total_monthly_savings=$((s3_cost + cloudfront_cost + dynamodb_cost + kms_cost))

    log_success "Estimated monthly cost savings: \$${total_monthly_savings} USD"
    log_info "Note: This is a rough estimate. Actual costs depend on usage, data transfer, and storage."
}

# Generate comprehensive dry run report
generate_dry_run_report() {
    log_info "ðŸ“‹ Generating comprehensive dry run report..."

    local report_file="/tmp/destruction-report-$(date +%Y%m%d-%H%M%S).txt"
    local total_resources=0

    # Set timeout for long-running operations
    local AWS_CLI_TIMEOUT="timeout 10"

    {
        echo "==============================================="
        echo "AWS Infrastructure Destruction Report"
        echo "Generated: $(date)"
        echo "Account: $(aws sts get-caller-identity --query 'Account' --output text 2>/dev/null || echo 'Unknown')"
        echo "Region: $AWS_DEFAULT_REGION"
        echo "Cross-Account Mode: $INCLUDE_CROSS_ACCOUNT"
        echo "Member Account Closure: $CLOSE_MEMBER_ACCOUNTS"
        echo "Terraform State Cleanup: $CLEANUP_TERRAFORM_STATE"
        echo "==============================================="
        echo ""
        echo "RESOURCES THAT WOULD BE DESTROYED:"
        echo ""

        # Cross-Account Roles
        if [[ "$INCLUDE_CROSS_ACCOUNT" == "true" ]]; then
            echo "ðŸ” CROSS-ACCOUNT ROLES:"
            local current_account
            current_account=$(aws sts get-caller-identity --query 'Account' --output text 2>/dev/null)

            if [[ "$current_account" == "$MANAGEMENT_ACCOUNT_ID" ]]; then
                local -A account_env_map=(
                    ["822529998967"]="Dev"
                    ["927588814642"]="Staging"
                    ["546274483801"]="Prod"
                )

                local cross_account_count=0
                for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
                    if check_account_filter "$account_id"; then
                        local env_name="${account_env_map[$account_id]}"
                        echo "  - GitHubActions-StaticSite-${env_name}-Role in account $account_id"
                        ((cross_account_count++)) || true
                    fi
                done
                echo "  Total: $cross_account_count cross-account roles"
                ((total_resources += cross_account_count)) || true
            else
                echo "  - Cross-account destruction requires management account access"
                echo "  Total: 0 cross-account roles (wrong account)"
            fi
            echo ""
        fi

        # S3 Buckets
        echo "ðŸª£ S3 BUCKETS:"
        local buckets
        buckets=$($AWS_CLI_TIMEOUT AWS_DEFAULT_REGION=us-east-1 aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)
        local bucket_count=0
        for bucket in $buckets; do
            if matches_project "$bucket"; then
                local size
                size=$(aws s3 ls "s3://$bucket" --recursive --summarize 2>/dev/null | grep "Total Size:" | cut -d: -f2 | xargs || echo "Unknown")
                echo "  - $bucket (Size: $size bytes)"
                ((bucket_count++)) || true
            fi
        done
        echo "  Total: $bucket_count buckets"
        ((total_resources += bucket_count)) || true
        echo ""

        # CloudFront Distributions
        echo "ðŸŒ CLOUDFRONT DISTRIBUTIONS:"
        local distributions
        distributions=$($AWS_CLI_TIMEOUT aws cloudfront list-distributions --query 'DistributionList.Items[].{Id:Id,Comment:Comment,DomainName:DomainName}' --output json 2>/dev/null || echo "[]")
        local cf_count=0
        if [[ "$distributions" != "[]" ]] && [[ "$distributions" != "null" ]] && [[ -n "$distributions" ]]; then
            echo "$distributions" | jq -r '.[] | select(.Comment != null) | "  - " + .Id + " (" + .Comment + ") - " + .DomainName' | while read -r line; do
                if [[ -n "$line" ]]; then
                    echo "$line"
                    ((cf_count++)) || true
                fi
            done
            cf_count=$(echo "$distributions" | jq '. | length' 2>/dev/null || echo 0)
        else
            cf_count=0
        fi
        echo "  Total: $cf_count distributions"
        ((total_resources += cf_count)) || true
        echo ""

        # DynamoDB Tables
        echo "ðŸ—ƒï¸ DYNAMODB TABLES:"
        local tables
        tables=$(aws dynamodb list-tables --query 'TableNames[]' --output text 2>/dev/null || true)
        local table_count=0
        for table in $tables; do
            if matches_project "$table"; then
                echo "  - $table"
                ((table_count++)) || true
            fi
        done
        echo "  Total: $table_count tables"
        ((total_resources += table_count)) || true
        echo ""

        # KMS Keys
        echo "ðŸ” KMS KEYS:"
        local aliases
        aliases=$(aws kms list-aliases --query 'Aliases[].{AliasName:AliasName,TargetKeyId:TargetKeyId}' --output json 2>/dev/null || echo "[]")
        local kms_count=0
        if [[ "$aliases" != "[]" ]] && [[ "$aliases" != "null" ]] && [[ -n "$aliases" ]]; then
            echo "$aliases" | jq -c '.[]' | while read -r alias_info; do
            local alias_name
            alias_name=$(echo "$alias_info" | jq -r '.AliasName')
            if matches_project "$alias_name"; then
                echo "  - $alias_name"
                    ((kms_count++)) || true
                fi
            done
        fi
        echo "  Total: $kms_count keys"
        ((total_resources += kms_count)) || true
        echo ""

        # IAM Resources
        echo "ðŸ‘¤ IAM RESOURCES:"
        local roles
        roles=$(aws iam list-roles --query 'Roles[].RoleName' --output text 2>/dev/null || true)
        local role_count=0
        echo "  Roles:"
        for role in $roles; do
            if matches_project "$role"; then
                echo "    - $role"
                ((role_count++)) || true
            fi
        done

        local policies
        policies=$(aws iam list-policies --scope Local --query 'Policies[].PolicyName' --output text 2>/dev/null || true)
        local policy_count=0
        echo "  Policies:"
        for policy in $policies; do
            if matches_project "$policy"; then
                echo "    - $policy"
                ((policy_count++)) || true
            fi
        done

        local oidc_count
        oidc_count=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[].Arn' --output text 2>/dev/null | grep -c "token.actions.githubusercontent.com" || echo 0)
        # Ensure oidc_count is a single number
        oidc_count=$(echo "$oidc_count" | head -1 | tr -d '[:space:]')
        [[ ! "$oidc_count" =~ ^[0-9]+$ ]] && oidc_count=0
        echo "  OIDC Providers: $oidc_count"
        echo "  Total: $((role_count + policy_count + oidc_count)) IAM resources"
        ((total_resources += role_count + policy_count + oidc_count)) || true
        echo ""

        # CloudWatch Resources
        echo "ðŸ“Š CLOUDWATCH RESOURCES:"
        local log_groups
        log_groups=$(aws logs describe-log-groups --query 'logGroups[].logGroupName' --output text 2>/dev/null || true)
        local lg_count=0
        echo "  Log Groups:"
        for lg in $log_groups; do
            if matches_project "$lg" || [[ "$lg" == *"/aws/cloudtrail"* ]]; then
                echo "    - $lg"
                ((lg_count++)) || true
            fi
        done

        local alarms
        alarms=$(aws cloudwatch describe-alarms --query 'MetricAlarms[].AlarmName' --output text 2>/dev/null || true)
        local alarm_count=0
        echo "  Alarms:"
        for alarm in $alarms; do
            if matches_project "$alarm"; then
                echo "    - $alarm"
                ((alarm_count++)) || true
            fi
        done
        echo "  Total: $((lg_count + alarm_count)) CloudWatch resources"
        ((total_resources += lg_count + alarm_count)) || true
        echo ""

        # Summary
        echo "==============================================="
        echo "SUMMARY:"
        echo "  Total resources to be destroyed: $total_resources"
        echo "  Estimated monthly cost savings: ~\$$(generate_cost_estimate_value) USD"
        echo "==============================================="

    } | tee "$report_file"

    log_success "Dry run report saved to: $report_file"
    return 0
}

# Helper function for cost estimate value only
generate_cost_estimate_value() {
    local s3_buckets_count
    s3_buckets_count=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null | wc -w || echo 0)

    local cloudfront_count
    cloudfront_count=$(aws cloudfront list-distributions --query 'DistributionList.Items[].Id' --output text 2>/dev/null | wc -w || echo 0)

    # Ensure numeric values
    s3_buckets_count=${s3_buckets_count:-0}
    cloudfront_count=${cloudfront_count:-0}

    # Convert to numeric if they contain whitespace
    s3_buckets_count=$(echo "$s3_buckets_count" | tr -d '[:space:]')
    cloudfront_count=$(echo "$cloudfront_count" | tr -d '[:space:]')

    # Ensure they're valid numbers
    [[ ! "$s3_buckets_count" =~ ^[0-9]+$ ]] && s3_buckets_count=0
    [[ ! "$cloudfront_count" =~ ^[0-9]+$ ]] && cloudfront_count=0

    local s3_cost=$((s3_buckets_count * 5))
    local cloudfront_cost=$((cloudfront_count * 10))
    local dynamodb_cost=5
    local kms_cost=10

    echo $((s3_cost + cloudfront_cost + dynamodb_cost + kms_cost))
}

# Parse command line arguments
parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --dry-run)
                DRY_RUN=true
                shift
                ;;
            --force)
                FORCE_DESTROY=true
                shift
                ;;
            --account-filter)
                ACCOUNT_FILTER="$2"
                shift 2
                ;;
            --region)
                AWS_DEFAULT_REGION="$2"
                shift 2
                ;;
            --no-cross-account)
                INCLUDE_CROSS_ACCOUNT=false
                shift
                ;;
            --close-accounts)
                CLOSE_MEMBER_ACCOUNTS=true
                shift
                ;;
            --no-terraform-cleanup)
                CLEANUP_TERRAFORM_STATE=false
                shift
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option: $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# Show help message
show_help() {
    cat << EOF
Usage: $SCRIPT_NAME [OPTIONS]

Destroy all AWS infrastructure created by the static-site repository including
cross-account resources, IAM/auth resources, and optionally member accounts.

OPTIONS:
    --dry-run                 Show what would be destroyed without actually doing it
    --force                   Skip all confirmation prompts (use with extreme caution)
    --account-filter IDS      Comma-separated list of AWS account IDs to limit destruction
    --region REGION           AWS region (default: us-east-1)
    --no-cross-account        Disable cross-account role destruction
    --close-accounts          Enable member account closure (PERMANENT)
    --no-terraform-cleanup    Disable Terraform state cleanup
    -h, --help               Show this help message

CROSS-ACCOUNT FEATURES:
    â€¢ Destroys GitHub Actions roles across all member accounts
    â€¢ Cleans up cross-account Terraform state
    â€¢ Requires management account access (${MANAGEMENT_ACCOUNT_ID})
    â€¢ Uses OrganizationAccountAccessRole for cross-account access

MEMBER ACCOUNTS:
    â€¢ Dev Account:     822529998967
    â€¢ Staging Account: 927588814642
    â€¢ Prod Account:    546274483801

EXAMPLES:
    # Dry run to see what would be destroyed (recommended first)
    $SCRIPT_NAME --dry-run

    # Complete infrastructure destruction including cross-account
    $SCRIPT_NAME --force

    # Destroy only specific accounts
    $SCRIPT_NAME --account-filter "822529998967,927588814642" --dry-run

    # Full destruction including member account closure (EXTREME)
    $SCRIPT_NAME --force --close-accounts

    # Disable cross-account features
    $SCRIPT_NAME --dry-run --no-cross-account

    # Cleanup current account only, no state cleanup
    $SCRIPT_NAME --no-cross-account --no-terraform-cleanup

ENVIRONMENT VARIABLES:
    AWS_DEFAULT_REGION        AWS region (default: us-east-1)
    FORCE_DESTROY            Set to 'true' to skip confirmations
    DRY_RUN                  Set to 'true' for dry run mode
    ACCOUNT_FILTER           Comma-separated AWS account IDs
    INCLUDE_CROSS_ACCOUNT    Set to 'false' to disable cross-account destruction
    CLOSE_MEMBER_ACCOUNTS    Set to 'true' to enable account closure
    CLEANUP_TERRAFORM_STATE  Set to 'false' to disable state cleanup

DESTRUCTION PHASES:
    Phase 1:  Cross-account infrastructure cleanup
    Phase 2:  Dependent resources (CloudFront, WAF)
    Phase 3:  Storage and logging (S3 multi-region, CloudTrail, CloudWatch, SNS)
    Phase 4:  Compute and database (DynamoDB)
    Phase 5:  DNS and network (Route53 zones, health checks, records)
    Phase 6:  Identity and security (IAM roles/users/groups, KMS)
    Phase 7:  Cost and configuration (Budgets, SSM Parameters)
    Phase 8:  Orphaned resources cleanup (Elastic IPs, etc.)
    Phase 9:  AWS Organizations cleanup (SCPs, OUs) - management account only
    Phase 10: Member account closure (if enabled) - PERMANENT for 90 days
    Phase 11: Post-destruction validation across all US regions

SAFETY FEATURES:
    â€¢ Dry run mode shows complete destruction plan
    â€¢ Individual confirmation for each resource type
    â€¢ Account filtering to limit scope
    â€¢ Enhanced logging and progress tracking
    â€¢ Cross-account access validation

WARNING - PERMANENT DATA LOSS:
    This script will PERMANENTLY DELETE all matching AWS resources including:
    â€¢ All S3 buckets and contents (including replicas across US regions)
    â€¢ All S3 replication configurations and intelligent tiering
    â€¢ All IAM roles, policies, users, groups, and OIDC providers
    â€¢ All KMS keys (scheduled for deletion)
    â€¢ All CloudFront distributions and associated resources
    â€¢ All DynamoDB tables and Terraform state locks
    â€¢ All CloudWatch dashboards, alarms, and composite alarms
    â€¢ All Route53 hosted zones, health checks, and DNS records
    â€¢ All AWS Budgets and budget actions
    â€¢ All SSM Parameter Store parameters
    â€¢ All CloudTrail trails and organization trails
    â€¢ All AWS Organizations resources (SCPs, OUs) - management account only
    â€¢ Terraform state for cross-account modules
    â€¢ Optionally: Member accounts (90-day closure period)

    MULTI-REGION: Scans all US regions (us-east-1, us-east-2, us-west-1, us-west-2)
    USE --dry-run FIRST to review the complete destruction plan.
    Member account closure cannot be undone for 90 days.
EOF
}

# Post-destruction validation
validate_complete_destruction() {
    log_info "ðŸ” Validating complete destruction across all regions..."

    local remaining_resources=0
    local regions
    regions=$(get_us_regions)

    echo "" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
    echo "## ðŸ” Post-Destruction Validation" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
    echo "" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true

    for region in $regions; do
        log_info "Validating region: $region"

        # Check S3 buckets
        local s3_count=0
        local buckets
        buckets=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)
        for bucket in $buckets; do
            if matches_project "$bucket"; then
                ((s3_count++)) || true
                log_warn "  Found remaining S3 bucket: $bucket"
            fi
        done

        # Check DynamoDB tables
        local dynamo_count=0
        local tables
        tables=$(AWS_DEFAULT_REGION=$region aws dynamodb list-tables --query 'TableNames[]' --output text 2>/dev/null || true)
        for table in $tables; do
            if matches_project "$table"; then
                ((dynamo_count++)) || true
                log_warn "  Found remaining DynamoDB table: $table (region: $region)"
            fi
        done

        # Check CloudWatch log groups
        local log_count=0
        local log_groups
        log_groups=$(AWS_DEFAULT_REGION=$region aws logs describe-log-groups --query 'logGroups[].logGroupName' --output text 2>/dev/null || true)
        for log_group in $log_groups; do
            if matches_project "$log_group" || [[ "$log_group" == *"/aws/cloudtrail"* ]]; then
                ((log_count++)) || true
                log_warn "  Found remaining log group: $log_group (region: $region)"
            fi
        done

        remaining_resources=$((remaining_resources + s3_count + dynamo_count + log_count))
    done

    # Check global resources
    local cf_count=0
    local distributions
    distributions=$(aws cloudfront list-distributions --query 'DistributionList.Items[].Id' --output text 2>/dev/null || true)
    for dist_id in $distributions; do
        ((cf_count++)) || true
        log_warn "Found remaining CloudFront distribution: $dist_id"
    done

    local iam_roles_count=0
    local roles
    roles=$(aws iam list-roles --query 'Roles[].RoleName' --output text 2>/dev/null || true)
    for role in $roles; do
        if matches_project "$role"; then
            ((iam_roles_count++)) || true
            log_warn "Found remaining IAM role: $role"
        fi
    done

    remaining_resources=$((remaining_resources + cf_count + iam_roles_count))

    echo "| Resource Type | Remaining Count |" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
    echo "|--------------|----------------|" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
    echo "| **Total** | **$remaining_resources** |" >> $GITHUB_STEP_SUMMARY 2>/dev/null || true

    if [[ $remaining_resources -eq 0 ]]; then
        log_success "âœ… Complete destruction validated - no remaining resources found"
        return 0
    else
        log_warn "âš ï¸ Found $remaining_resources remaining resources"
        log_warn "Review the warnings above and run the script again if needed"
        return 0
    fi
}

# Main execution function
main() {
    local start_time
    start_time=$(date +%s)

    # Only show banner if not in force mode
    if [[ "$FORCE_DESTROY" != "true" ]]; then
        echo -e "${BOLD}${RED}"
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘                    ðŸš¨ DANGER ZONE ðŸš¨                        â•‘"
        echo "â•‘                                                              â•‘"
        echo "â•‘  This script will PERMANENTLY DELETE all AWS resources      â•‘"
        echo "â•‘  created by the static-site infrastructure repository.      â•‘"
        echo "â•‘                                                              â•‘"
        echo "â•‘  Resources that will be destroyed:                          â•‘"
        echo "â•‘  â€¢ S3 buckets (all US regions) and contents               â•‘"
        echo "â•‘  â€¢ KMS keys (scheduled for deletion)                       â•‘"
        echo "â•‘  â€¢ IAM roles, users, groups, policies, OIDC providers      â•‘"
        echo "â•‘  â€¢ CloudFront distributions                                 â•‘"
        echo "â•‘  â€¢ CloudWatch dashboards, alarms, log groups               â•‘"
        echo "â•‘  â€¢ Route53 zones, health checks, DNS records              â•‘"
        echo "â•‘  â€¢ DynamoDB tables                                          â•‘"
        echo "â•‘  â€¢ AWS Budgets and SSM parameters                          â•‘"
        echo "â•‘  â€¢ Organizations resources (SCPs, OUs)                     â•‘"
        echo "â•‘  â€¢ All other project-related AWS resources                 â•‘"
        echo "â•‘                                                              â•‘"
        echo "â•‘  ðŸ’¸ This action may result in significant cost savings      â•‘"
        echo "â•‘  ðŸ’€ This action CANNOT be undone                           â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo -e "${NC}"
    fi

    log_info "Starting infrastructure destruction script"
    log_info "Log file: $LOG_FILE"
    log_info "Dry run mode: $DRY_RUN"
    log_info "Force mode: $FORCE_DESTROY"
    log_info "AWS Region: $AWS_DEFAULT_REGION"

    if [[ -n "$ACCOUNT_FILTER" ]]; then
        log_info "Account filter: $ACCOUNT_FILTER"
    fi

    # Verify AWS CLI is configured
    if ! aws sts get-caller-identity >/dev/null 2>&1; then
        log_error "AWS CLI is not configured or lacks permissions"
        log_error "Please run 'aws configure' or set up AWS credentials"
        exit 3
    fi

    local current_account current_region
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)
    current_region=$(aws configure get region || echo "$AWS_DEFAULT_REGION")

    log_info "Current AWS Account: $current_account"
    log_info "Current AWS Region: $current_region"

    # Final confirmation
    if [[ "$FORCE_DESTROY" != "true" ]] && [[ "$DRY_RUN" != "true" ]]; then
        echo
        echo -e "${RED}${BOLD}FINAL WARNING:${NC}"
        echo "You are about to destroy ALL infrastructure in AWS account: $current_account"
        echo "This includes PERMANENT deletion of data and resources."
        echo
        read -p "Type 'DESTROY EVERYTHING' to confirm: " final_confirmation

        if [[ "$final_confirmation" != "DESTROY EVERYTHING" ]]; then
            log_warn "Operation cancelled by user"
            exit 2
        fi
    fi

    # If dry run, generate report and exit
    if [[ "$DRY_RUN" == "true" ]]; then
        log_info "Running in DRY RUN mode - no resources will be destroyed"
        generate_dry_run_report

        local end_time duration
        end_time=$(date +%s)
        duration=$((end_time - start_time))

        log_success "Dry run completed in ${duration} seconds"
        log_info "Review the report above to see what would be destroyed"
        log_info "To perform actual destruction, run without --dry-run"
        exit 0
    fi

    log_info "Beginning destruction sequence..."

    # Track destruction results
    declare -A destruction_results
    local total_destroyed=0
    local total_failed=0

    # Execute destruction in order (dependent resources first)
    log_info "Phase 1: Cross-account infrastructure cleanup..."
    destroy_cross_account_roles
    cleanup_terraform_state

    log_info "Phase 2: Destroying dependent resources..."
    destroy_cloudfront_distributions
    destroy_waf_resources

    log_info "Phase 3: Destroying storage and logging (multi-region)..."
    destroy_s3_buckets
    log_info "Destroying replica S3 buckets in us-west-2..."
    destroy_replica_s3_buckets "us-west-2"
    destroy_cloudtrail_resources
    destroy_cloudwatch_resources
    destroy_cloudwatch_dashboards
    destroy_sns_resources

    log_info "Phase 4: Destroying compute and database resources..."
    destroy_dynamodb_tables

    log_info "Phase 5: Destroying DNS and network resources..."
    destroy_route53_resources

    log_info "Phase 6: Destroying identity and security..."
    destroy_iam_resources
    destroy_kms_keys

    log_info "Phase 7: Destroying cost and configuration management..."
    destroy_aws_budgets
    destroy_ssm_parameters

    log_info "Phase 8: Cleanup orphaned resources..."
    cleanup_orphaned_resources

    log_info "Phase 9: AWS Organizations cleanup (if enabled)..."
    destroy_organizations_resources

    log_info "Phase 10: Member account closure (if enabled)..."
    close_member_accounts

    # Generate cost savings estimate
    generate_cost_estimate

    # Validate complete destruction
    log_info "Phase 11: Post-destruction validation..."
    validate_complete_destruction

    local end_time duration
    end_time=$(date +%s)
    duration=$((end_time - start_time))

    log_success "Infrastructure destruction completed in ${duration} seconds"
    log_success "Log file saved: $LOG_FILE"

    echo
    log_success "ðŸŽ‰ All infrastructure has been destroyed!"
    log_success "ðŸ’° You should see cost savings on your next AWS bill"
    echo
    log_warn "Note: Some resources like KMS keys have mandatory waiting periods"
    log_warn "Check the AWS console to verify all resources are gone"

    # Generate final summary
    echo
    echo -e "${BOLD}${GREEN}DESTRUCTION SUMMARY:${NC}"
    echo "  Total resources destroyed: ${total_destroyed:-unknown}"
    echo "  Failed destructions: ${total_failed:-0}"
    echo "  Log file: $LOG_FILE"
}

# Script entry point
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    # Parse command line arguments
    parse_arguments "$@"

    # Run main function
    main
fi