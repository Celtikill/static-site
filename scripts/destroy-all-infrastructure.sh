#!/usr/bin/env bash
#
# Script: destroy-all-infrastructure.sh
# Purpose: Complete destruction of all AWS infrastructure created by this repository
# Author: Generated by Claude for static-site infrastructure cleanup
#
# DANGER: This script will PERMANENTLY DELETE all AWS resources created by this repository.
# This includes SINGLE-ACCOUNT and CROSS-ACCOUNT resources:
#
# CROSS-ACCOUNT RESOURCES (requires management account access):
# - GitHub Actions roles in all member accounts (dev, staging, prod)
# - Cross-account Terraform state cleanup
# - Member account closure (optional, PERMANENT for 90 days)
#
# SINGLE-ACCOUNT RESOURCES:
# - All S3 buckets and their contents
# - All KMS keys (scheduled for deletion)
# - All IAM roles, policies, and OIDC providers
# - All CloudFront distributions
# - All DynamoDB tables (Terraform state locks)
# - All CloudTrail trails and logs
# - All SNS topics and subscriptions
# - All CloudWatch alarms and log groups
# - All WAF Web ACLs
# - All Route53 hosted zones (if created)
# - All ACM certificates
# - Any orphaned EC2 resources (Elastic IPs, etc.)
#
# Required Environment Variables:
#   - AWS_DEFAULT_REGION: AWS region (default: us-east-1)
# Optional Environment Variables:
#   - FORCE_DESTROY: Set to 'true' to skip confirmation prompts
#   - DRY_RUN: Set to 'true' to see what would be destroyed without doing it
#   - ACCOUNT_FILTER: Comma-separated list of account IDs to limit destruction to
#   - INCLUDE_CROSS_ACCOUNT: Set to 'false' to disable cross-account destruction
#   - CLOSE_MEMBER_ACCOUNTS: Set to 'true' to enable member account closure
#   - CLEANUP_TERRAFORM_STATE: Set to 'false' to disable Terraform state cleanup
#
# Usage:
#   # Dry run mode (recommended first step)
#   ./destroy-all-infrastructure.sh --dry-run
#
#   # Complete infrastructure destruction with cross-account cleanup
#   ./destroy-all-infrastructure.sh --force
#
#   # Destroy specific accounts only
#   ./destroy-all-infrastructure.sh --account-filter "822529998967,927588814642" --dry-run
#
#   # Full destruction including member account closure (EXTREME)
#   ./destroy-all-infrastructure.sh --force --close-accounts
#
#   # Single account cleanup only (no cross-account features)
#   ./destroy-all-infrastructure.sh --no-cross-account --no-terraform-cleanup
#
# Exit Codes:
#   0 - Success (all resources destroyed)
#   1 - General failure
#   2 - User cancelled operation
#   3 - AWS CLI not configured
#   4 - Missing required permissions

set -euo pipefail

# Constants and Configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly SCRIPT_NAME="$(basename "${BASH_SOURCE[0]}")"
readonly LOG_FILE="/tmp/destroy-infrastructure-$(date +%Y%m%d-%H%M%S).log"

# Default values
: "${AWS_DEFAULT_REGION:=us-east-1}"
: "${FORCE_DESTROY:=false}"
: "${DRY_RUN:=false}"
: "${ACCOUNT_FILTER:=}"
: "${INCLUDE_CROSS_ACCOUNT:=true}"
: "${CLOSE_MEMBER_ACCOUNTS:=false}"
: "${CLEANUP_TERRAFORM_STATE:=true}"

# Colors for output (if terminal supports it)
if [[ -t 1 ]]; then
    readonly RED='\033[0;31m'
    readonly GREEN='\033[0;32m'
    readonly YELLOW='\033[1;33m'
    readonly BLUE='\033[0;34m'
    readonly BOLD='\033[1m'
    readonly NC='\033[0m' # No Color
else
    readonly RED=''
    readonly GREEN=''
    readonly YELLOW=''
    readonly BLUE=''
    readonly BOLD=''
    readonly NC=''
fi

# Project-specific resource patterns
readonly PROJECT_PATTERNS=(
    "static-site"
    "StaticSite"
    "terraform-state"
    "GitHubActions"
    "cloudtrail-logs"
)

# AWS Organization Account IDs (from organization-management.yml)
readonly MEMBER_ACCOUNT_IDS=(
    "822529998967"  # dev
    "927588814642"  # staging
    "546274483801"  # prod
)

readonly MANAGEMENT_ACCOUNT_ID="223938610551"
readonly EXTERNAL_ID="github-actions-static-site"

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "$LOG_FILE"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$LOG_FILE"
}

log_action() {
    if [[ "$DRY_RUN" == "true" ]]; then
        echo -e "${YELLOW}[DRY RUN]${NC} Would $1" | tee -a "$LOG_FILE"
    else
        echo -e "${BOLD}[ACTION]${NC} $1" | tee -a "$LOG_FILE"
    fi
}

# Confirmation function
confirm_destruction() {
    local resource_type="$1"
    local resource_name="$2"

    if [[ "$FORCE_DESTROY" == "true" ]] || [[ "$DRY_RUN" == "true" ]]; then
        return 0
    fi

    echo -e "${RED}${BOLD}DANGER:${NC} About to destroy ${YELLOW}$resource_type${NC}: ${BOLD}$resource_name${NC}"

    # Add timeout for non-interactive environments
    local confirmation
    if read -t 30 -p "Are you sure? (type 'DELETE' to confirm): " confirmation; then
        if [[ "$confirmation" != "DELETE" ]]; then
            log_warn "Skipping $resource_type: $resource_name"
            return 1
        fi
    else
        log_warn "Timeout waiting for confirmation - skipping $resource_type: $resource_name"
        return 1
    fi
    return 0
}

# Check if resource matches project patterns
matches_project() {
    local resource_name="$1"
    local pattern

    # Handle null or empty resource names
    if [[ -z "$resource_name" ]] || [[ "$resource_name" == "null" ]]; then
        return 1
    fi

    for pattern in "${PROJECT_PATTERNS[@]}"; do
        if [[ "$resource_name" == *"$pattern"* ]]; then
            return 0
        fi
    done
    return 1
}

# Check account filter
check_account_filter() {
    local account_id="$1"

    if [[ -z "$ACCOUNT_FILTER" ]]; then
        return 0  # No filter, allow all
    fi

    IFS=',' read -ra ACCOUNTS <<< "$ACCOUNT_FILTER"
    for allowed_account in "${ACCOUNTS[@]}"; do
        if [[ "$account_id" == "$allowed_account" ]]; then
            return 0
        fi
    done
    return 1
}

# AWS CLI wrapper with error handling
aws_cmd() {
    local cmd=("$@")

    if [[ "$DRY_RUN" == "true" ]]; then
        log_action "Run: aws ${cmd[*]}"
        return 0
    fi

    # Add retry logic for transient failures
    local max_retries=3
    local retry_count=0

    while [[ $retry_count -lt $max_retries ]]; do
        if aws "${cmd[@]}" 2>>"$LOG_FILE"; then
            return 0
        fi

        ((retry_count++))
        if [[ $retry_count -lt $max_retries ]]; then
            log_warn "Command failed, retrying (${retry_count}/${max_retries}): aws ${cmd[*]}"
            sleep 2
        fi
    done

    log_error "Failed to execute after ${max_retries} attempts: aws ${cmd[*]}"
    return 1
}

# Destroy S3 buckets and contents
destroy_s3_buckets() {
    log_info "🪣 Scanning for S3 buckets..."

    local buckets
    # S3 buckets are global, no need for region-specific calls
    buckets=$(AWS_DEFAULT_REGION=us-east-1 aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)

    if [[ -z "$buckets" ]]; then
        log_info "No S3 buckets found"
        return 0
    fi

    for bucket in $buckets; do
        if matches_project "$bucket"; then
            if confirm_destruction "S3 Bucket" "$bucket"; then
                log_action "Empty and delete S3 bucket: $bucket"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Empty bucket first (including all versions and delete markers)
                    aws s3api list-object-versions --bucket "$bucket" \
                        --query 'Versions[].{Key:Key,VersionId:VersionId}' \
                        --output text 2>/dev/null | \
                        while read -r key version_id; do
                            [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Delete delete markers
                    aws s3api list-object-versions --bucket "$bucket" \
                        --query 'DeleteMarkers[].{Key:Key,VersionId:VersionId}' \
                        --output text 2>/dev/null | \
                        while read -r key version_id; do
                            [[ -n "$key" ]] && aws s3api delete-object --bucket "$bucket" --key "$key" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Force empty using CLI (as backup)
                    aws s3 rm "s3://$bucket" --recursive 2>/dev/null || true

                    # Delete bucket
                    if aws s3api delete-bucket --bucket "$bucket" 2>/dev/null; then
                        log_success "Deleted S3 bucket: $bucket"
                    else
                        log_error "Failed to delete S3 bucket: $bucket"
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudFront distributions
destroy_cloudfront_distributions() {
    log_info "🌐 Scanning for CloudFront distributions..."

    local distributions
    distributions=$(aws cloudfront list-distributions --query 'DistributionList.Items[].{Id:Id,DomainName:DomainName,Comment:Comment,Status:Status}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$distributions" == "null" ]] || [[ "$distributions" == "[]" ]] || [[ -z "$distributions" ]]; then
        log_info "No CloudFront distributions found"
        return 0
    fi

    echo "$distributions" | jq -c '.[]' | while read -r distribution; do
        local dist_id
        local comment
        local status

        dist_id=$(echo "$distribution" | jq -r '.Id')
        comment=$(echo "$distribution" | jq -r '.Comment // ""')
        status=$(echo "$distribution" | jq -r '.Status')

        if matches_project "$comment" || matches_project "$dist_id"; then
            if confirm_destruction "CloudFront Distribution" "$dist_id ($comment)"; then
                log_action "Disable and delete CloudFront distribution: $dist_id"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Get current config
                    local config etag
                    config=$(aws cloudfront get-distribution-config --id "$dist_id" --query 'DistributionConfig' --output json)
                    etag=$(aws cloudfront get-distribution-config --id "$dist_id" --query 'ETag' --output text)

                    # Disable distribution if enabled
                    if [[ "$(echo "$config" | jq -r '.Enabled')" == "true" ]]; then
                        config=$(echo "$config" | jq '.Enabled = false')

                        if aws cloudfront update-distribution --id "$dist_id" --distribution-config "$config" --if-match "$etag" >/dev/null; then
                            log_info "Distribution $dist_id disabled, waiting for deployment..."

                            # Wait for distribution to be deployed
                            local attempts=0
                            while [[ $attempts -lt 30 ]]; do
                                status=$(aws cloudfront get-distribution --id "$dist_id" --query 'Distribution.Status' --output text)
                                if [[ "$status" == "Deployed" ]]; then
                                    break
                                fi
                                log_info "Waiting for distribution $dist_id to be deployed (status: $status)..."
                                sleep 30
                                ((attempts++))
                            done
                        fi
                    fi

                    # Delete distribution (only works when disabled and deployed)
                    etag=$(aws cloudfront get-distribution --id "$dist_id" --query 'ETag' --output text)
                    if aws cloudfront delete-distribution --id "$dist_id" --if-match "$etag" 2>/dev/null; then
                        log_success "Deleted CloudFront distribution: $dist_id"
                    else
                        log_warn "Could not delete CloudFront distribution $dist_id (may need manual intervention)"
                    fi
                fi
            fi
        fi
    done
}

# Destroy DynamoDB tables
destroy_dynamodb_tables() {
    log_info "🗃️  Scanning for DynamoDB tables..."

    local tables
    tables=$(aws dynamodb list-tables --query 'TableNames[]' --output text 2>/dev/null || true)

    if [[ -z "$tables" ]]; then
        log_info "No DynamoDB tables found"
        return 0
    fi

    for table in $tables; do
        if matches_project "$table"; then
            if confirm_destruction "DynamoDB Table" "$table"; then
                log_action "Delete DynamoDB table: $table"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws dynamodb delete-table --table-name "$table" >/dev/null 2>&1; then
                        log_success "Deleted DynamoDB table: $table"
                    else
                        log_error "Failed to delete DynamoDB table: $table"
                    fi
                fi
            fi
        fi
    done
}

# Destroy KMS keys
destroy_kms_keys() {
    log_info "🔐 Scanning for KMS keys..."

    # Get all aliases first (with timeout to prevent hanging)
    local aliases
    aliases=$(timeout 10 aws kms list-aliases --query 'Aliases[].{AliasName:AliasName,TargetKeyId:TargetKeyId}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$aliases" == "null" ]] || [[ "$aliases" == "[]" ]] || [[ -z "$aliases" ]]; then
        log_info "No KMS aliases found"
        return 0
    fi

    echo "$aliases" | jq -c '.[]' | while read -r alias_info; do
        local alias_name target_key_id
        alias_name=$(echo "$alias_info" | jq -r '.AliasName')
        target_key_id=$(echo "$alias_info" | jq -r '.TargetKeyId // ""')

        if matches_project "$alias_name" && [[ -n "$target_key_id" ]]; then
            if confirm_destruction "KMS Key" "$alias_name ($target_key_id)"; then
                log_action "Schedule KMS key deletion: $alias_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Delete alias first
                    if aws kms delete-alias --alias-name "$alias_name" 2>/dev/null; then
                        log_success "Deleted KMS alias: $alias_name"
                    fi

                    # Schedule key deletion (7 day minimum)
                    if aws kms schedule-key-deletion --key-id "$target_key_id" --pending-window-in-days 7 >/dev/null 2>&1; then
                        log_success "Scheduled KMS key deletion: $target_key_id (7 days)"
                    else
                        log_error "Failed to schedule KMS key deletion: $target_key_id"
                    fi
                fi
            fi
        fi
    done
}

# Destroy cross-account GitHub Actions roles
destroy_cross_account_roles() {
    log_info "🔐 Scanning for cross-account GitHub Actions roles..."

    if [[ "$INCLUDE_CROSS_ACCOUNT" != "true" ]]; then
        log_info "Cross-account destruction disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Cross-account role destruction only supported from management account ($MANAGEMENT_ACCOUNT_ID)"
        log_warn "Current account: $current_account - skipping cross-account cleanup"
        return 0
    fi

    # Map account IDs to environment names
    local -A account_env_map=(
        ["822529998967"]="Dev"
        ["927588814642"]="Staging"
        ["546274483801"]="Prod"
    )

    for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
        if ! check_account_filter "$account_id"; then
            log_info "Skipping account $account_id - not in account filter"
            continue
        fi

        local env_name="${account_env_map[$account_id]}"
        local role_name="GitHubActions-StaticSite-${env_name}-Role"

        # Also check for alternative role naming patterns
        local alt_role_name="github-actions-workload-deployment"
        local org_role_arn="arn:aws:iam::${account_id}:role/OrganizationAccountAccessRole"

        log_info "Processing account $account_id ($env_name environment)"

        if confirm_destruction "Cross-Account Role" "$role_name in account $account_id"; then
            log_action "Destroy cross-account role: $role_name"

            if [[ "$DRY_RUN" != "true" ]]; then
                # Assume OrganizationAccountAccessRole in target account
                local assume_output
                if assume_output=$(aws sts assume-role \
                    --role-arn "$org_role_arn" \
                    --role-session-name "destroy-cross-account-${account_id}" \
                    --query 'Credentials.{AccessKeyId:AccessKeyId,SecretAccessKey:SecretAccessKey,SessionToken:SessionToken}' \
                    --output json 2>/dev/null); then

                    # Extract credentials
                    local access_key secret_key session_token
                    access_key=$(echo "$assume_output" | jq -r '.AccessKeyId')
                    secret_key=$(echo "$assume_output" | jq -r '.SecretAccessKey')
                    session_token=$(echo "$assume_output" | jq -r '.SessionToken')

                    # Check for both role naming patterns
                    local found_role=""
                    if AWS_ACCESS_KEY_ID="$access_key" \
                       AWS_SECRET_ACCESS_KEY="$secret_key" \
                       AWS_SESSION_TOKEN="$session_token" \
                       aws iam get-role --role-name "$role_name" >/dev/null 2>&1; then
                        found_role="$role_name"
                    elif AWS_ACCESS_KEY_ID="$access_key" \
                         AWS_SECRET_ACCESS_KEY="$secret_key" \
                         AWS_SESSION_TOKEN="$session_token" \
                         aws iam get-role --role-name "$alt_role_name" >/dev/null 2>&1; then
                        found_role="$alt_role_name"
                    fi

                    if [[ -n "$found_role" ]]; then
                        log_info "Found role $found_role in account $account_id - proceeding with destruction"

                        # Detach managed policies
                        AWS_ACCESS_KEY_ID="$access_key" \
                        AWS_SECRET_ACCESS_KEY="$secret_key" \
                        AWS_SESSION_TOKEN="$session_token" \
                        aws iam list-attached-role-policies --role-name "$found_role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                            while read -r policy_arn; do
                                [[ -n "$policy_arn" ]] && AWS_ACCESS_KEY_ID="$access_key" \
                                AWS_SECRET_ACCESS_KEY="$secret_key" \
                                AWS_SESSION_TOKEN="$session_token" \
                                aws iam detach-role-policy --role-name "$found_role" --policy-arn "$policy_arn" 2>/dev/null || true
                            done

                        # Delete inline policies
                        AWS_ACCESS_KEY_ID="$access_key" \
                        AWS_SECRET_ACCESS_KEY="$secret_key" \
                        AWS_SESSION_TOKEN="$session_token" \
                        aws iam list-role-policies --role-name "$found_role" --query 'PolicyNames[]' --output text 2>/dev/null | \
                            while read -r policy_name; do
                                [[ -n "$policy_name" ]] && AWS_ACCESS_KEY_ID="$access_key" \
                                AWS_SECRET_ACCESS_KEY="$secret_key" \
                                AWS_SESSION_TOKEN="$session_token" \
                                aws iam delete-role-policy --role-name "$found_role" --policy-name "$policy_name" 2>/dev/null || true
                            done

                        # Delete the role
                        if AWS_ACCESS_KEY_ID="$access_key" \
                           AWS_SECRET_ACCESS_KEY="$secret_key" \
                           AWS_SESSION_TOKEN="$session_token" \
                           aws iam delete-role --role-name "$found_role" 2>/dev/null; then
                            log_success "Deleted cross-account role: $found_role in account $account_id"
                        else
                            log_error "Failed to delete cross-account role: $found_role in account $account_id"
                        fi
                    else
                        log_info "Role $role_name not found in account $account_id - skipping"
                    fi
                else
                    log_error "Failed to assume OrganizationAccountAccessRole in account $account_id"
                    log_error "Ensure the role exists and this account has permission to assume it"
                fi
            fi
        fi
    done
}

# Destroy IAM resources
destroy_iam_resources() {
    log_info "👤 Scanning for IAM resources..."

    # Get current account ID for validation
    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    if ! check_account_filter "$current_account"; then
        log_warn "Skipping IAM resources - account $current_account not in filter"
        return 0
    fi

    # Destroy IAM roles (with timeout to prevent hanging)
    local roles
    roles=$(timeout 15 aws iam list-roles --query 'Roles[].{RoleName:RoleName,Arn:Arn}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$roles" == "null" ]] || [[ "$roles" == "[]" ]] || [[ -z "$roles" ]]; then
        log_info "No IAM roles found"
    else
        echo "$roles" | jq -c '.[]' | while read -r role_info; do
        local role_name role_arn
        role_name=$(echo "$role_info" | jq -r '.RoleName')
        role_arn=$(echo "$role_info" | jq -r '.Arn')

        if matches_project "$role_name"; then
            if confirm_destruction "IAM Role" "$role_name"; then
                log_action "Delete IAM role: $role_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Detach managed policies
                    aws iam list-attached-role-policies --role-name "$role_name" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
                        while read -r policy_arn; do
                            [[ -n "$policy_arn" ]] && aws iam detach-role-policy --role-name "$role_name" --policy-arn "$policy_arn" 2>/dev/null || true
                        done

                    # Delete inline policies
                    aws iam list-role-policies --role-name "$role_name" --query 'PolicyNames[]' --output text 2>/dev/null | \
                        while read -r policy_name; do
                            [[ -n "$policy_name" ]] && aws iam delete-role-policy --role-name "$role_name" --policy-name "$policy_name" 2>/dev/null || true
                        done

                    # Delete role
                    if aws iam delete-role --role-name "$role_name" 2>/dev/null; then
                        log_success "Deleted IAM role: $role_name"
                    else
                        log_error "Failed to delete IAM role: $role_name"
                    fi
                fi
            fi
        fi
        done
    fi

    # Destroy custom IAM policies (with timeout)
    local policies
    policies=$(timeout 15 aws iam list-policies --scope Local --query 'Policies[].{PolicyName:PolicyName,Arn:Arn}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$policies" == "null" ]] || [[ "$policies" == "[]" ]] || [[ -z "$policies" ]]; then
        log_info "No custom IAM policies found"
    else
        echo "$policies" | jq -c '.[]' | while read -r policy_info; do
        local policy_name policy_arn
        policy_name=$(echo "$policy_info" | jq -r '.PolicyName')
        policy_arn=$(echo "$policy_info" | jq -r '.Arn')

        if matches_project "$policy_name"; then
            if confirm_destruction "IAM Policy" "$policy_name"; then
                log_action "Delete IAM policy: $policy_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Delete all policy versions except default
                    aws iam list-policy-versions --policy-arn "$policy_arn" --query 'Versions[?!IsDefaultVersion].VersionId' --output text 2>/dev/null | \
                        while read -r version_id; do
                            [[ -n "$version_id" ]] && aws iam delete-policy-version --policy-arn "$policy_arn" --version-id "$version_id" 2>/dev/null || true
                        done

                    # Delete policy
                    if aws iam delete-policy --policy-arn "$policy_arn" 2>/dev/null; then
                        log_success "Deleted IAM policy: $policy_name"
                    else
                        log_error "Failed to delete IAM policy: $policy_name"
                    fi
                fi
            fi
        fi
        done
    fi

    # Destroy OIDC identity providers
    local oidc_providers
    oidc_providers=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[].Arn' --output text 2>/dev/null || true)

    for provider_arn in $oidc_providers; do
        if [[ "$provider_arn" == *"token.actions.githubusercontent.com"* ]]; then
            if confirm_destruction "OIDC Identity Provider" "$provider_arn"; then
                log_action "Delete OIDC identity provider: $provider_arn"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws iam delete-open-id-connect-provider --open-id-connect-provider-arn "$provider_arn" 2>/dev/null; then
                        log_success "Deleted OIDC identity provider: $provider_arn"
                    else
                        log_error "Failed to delete OIDC identity provider: $provider_arn"
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudWatch resources
destroy_cloudwatch_resources() {
    log_info "📊 Scanning for CloudWatch resources..."

    # Destroy log groups
    local log_groups
    log_groups=$(aws logs describe-log-groups --query 'logGroups[].logGroupName' --output text 2>/dev/null || true)

    for log_group in $log_groups; do
        if matches_project "$log_group" || [[ "$log_group" == *"/aws/cloudtrail"* ]]; then
            if confirm_destruction "CloudWatch Log Group" "$log_group"; then
                log_action "Delete CloudWatch log group: $log_group"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws logs delete-log-group --log-group-name "$log_group" 2>/dev/null; then
                        log_success "Deleted CloudWatch log group: $log_group"
                    else
                        log_error "Failed to delete CloudWatch log group: $log_group"
                    fi
                fi
            fi
        fi
    done

    # Destroy alarms
    local alarms
    alarms=$(aws cloudwatch describe-alarms --query 'MetricAlarms[].AlarmName' --output text 2>/dev/null || true)

    for alarm in $alarms; do
        if matches_project "$alarm"; then
            if confirm_destruction "CloudWatch Alarm" "$alarm"; then
                log_action "Delete CloudWatch alarm: $alarm"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws cloudwatch delete-alarms --alarm-names "$alarm" 2>/dev/null; then
                        log_success "Deleted CloudWatch alarm: $alarm"
                    else
                        log_error "Failed to delete CloudWatch alarm: $alarm"
                    fi
                fi
            fi
        fi
    done
}

# Destroy SNS resources
destroy_sns_resources() {
    log_info "📢 Scanning for SNS resources..."

    local topics
    topics=$(aws sns list-topics --query 'Topics[].TopicArn' --output text 2>/dev/null || true)

    for topic_arn in $topics; do
        local topic_name
        topic_name=$(basename "$topic_arn")

        if matches_project "$topic_name"; then
            if confirm_destruction "SNS Topic" "$topic_name"; then
                log_action "Delete SNS topic: $topic_arn"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws sns delete-topic --topic-arn "$topic_arn" 2>/dev/null; then
                        log_success "Deleted SNS topic: $topic_name"
                    else
                        log_error "Failed to delete SNS topic: $topic_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy CloudTrail resources
destroy_cloudtrail_resources() {
    log_info "📋 Scanning for CloudTrail resources..."

    local trails
    trails=$(aws cloudtrail describe-trails --query 'trailList[].{Name:Name,S3BucketName:S3BucketName}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$trails" == "null" ]] || [[ "$trails" == "[]" ]] || [[ -z "$trails" ]]; then
        log_info "No CloudTrail trails found"
        return 0
    fi

    echo "$trails" | jq -c '.[]' | while read -r trail_info; do
        local trail_name s3_bucket
        trail_name=$(echo "$trail_info" | jq -r '.Name')
        s3_bucket=$(echo "$trail_info" | jq -r '.S3BucketName // ""')

        if matches_project "$trail_name" || matches_project "$s3_bucket"; then
            if confirm_destruction "CloudTrail Trail" "$trail_name"; then
                log_action "Delete CloudTrail trail: $trail_name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Stop logging first
                    aws cloudtrail stop-logging --name "$trail_name" 2>/dev/null || true

                    # Delete trail
                    if aws cloudtrail delete-trail --name "$trail_name" 2>/dev/null; then
                        log_success "Deleted CloudTrail trail: $trail_name"
                    else
                        log_error "Failed to delete CloudTrail trail: $trail_name"
                    fi
                fi
            fi
        fi
    done
}

# Destroy WAF resources
destroy_waf_resources() {
    log_info "🛡️  Scanning for WAF resources..."

    # Check CloudFront scope
    local web_acls
    web_acls=$(aws wafv2 list-web-acls --scope CLOUDFRONT --query 'WebACLs[].{Name:Name,Id:Id}' --output json 2>/dev/null || echo "[]")

    # Handle null or empty response
    if [[ "$web_acls" == "null" ]] || [[ "$web_acls" == "[]" ]] || [[ -z "$web_acls" ]]; then
        log_info "No WAF Web ACLs found"
        return 0
    fi

    echo "$web_acls" | jq -c '.[]' | while read -r web_acl; do
        local name id
        name=$(echo "$web_acl" | jq -r '.Name')
        id=$(echo "$web_acl" | jq -r '.Id')

        if matches_project "$name"; then
            if confirm_destruction "WAF Web ACL" "$name"; then
                log_action "Delete WAF Web ACL: $name"

                if [[ "$DRY_RUN" != "true" ]]; then
                    # Get lock token
                    local lock_token
                    lock_token=$(aws wafv2 get-web-acl --scope CLOUDFRONT --id "$id" --name "$name" --query 'LockToken' --output text 2>/dev/null || true)

                    if [[ -n "$lock_token" ]] && aws wafv2 delete-web-acl --scope CLOUDFRONT --id "$id" --name "$name" --lock-token "$lock_token" 2>/dev/null; then
                        log_success "Deleted WAF Web ACL: $name"
                    else
                        log_error "Failed to delete WAF Web ACL: $name"
                    fi
                fi
            fi
        fi
    done
}

# Cleanup Terraform state for cross-account modules
cleanup_terraform_state() {
    log_info "🗂️  Cleaning up Terraform state for cross-account modules..."

    if [[ "$CLEANUP_TERRAFORM_STATE" != "true" ]]; then
        log_info "Terraform state cleanup disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Terraform state cleanup only supported from management account"
        return 0
    fi

    # Check if we're in the terraform directory
    local terraform_dir="$SCRIPT_DIR/../terraform/foundations/org-management"
    if [[ ! -d "$terraform_dir" ]]; then
        log_warn "Terraform directory not found: $terraform_dir"
        return 0
    fi

    if confirm_destruction "Terraform State" "cross-account-roles module state"; then
        log_action "Clean Terraform state for cross-account-roles module"

        if [[ "$DRY_RUN" != "true" ]]; then
            pushd "$terraform_dir" >/dev/null 2>&1 || {
                log_error "Failed to change to terraform directory: $terraform_dir"
                return 1
            }

            # Initialize terraform if needed
            if [[ ! -d ".terraform" ]]; then
                log_info "Initializing Terraform..."
                if ! tofu init -upgrade; then
                    log_error "Failed to initialize Terraform"
                    popd >/dev/null 2>&1 || true
                    return 1
                fi
            fi

            # List state resources related to cross-account roles
            log_info "Checking for cross-account role resources in state..."
            local state_resources
            state_resources=$(tofu state list 2>/dev/null | grep -E "(cross_account|cross-account)" || true)

            if [[ -n "$state_resources" ]]; then
                log_info "Found cross-account resources in state:"
                echo "$state_resources" | while read -r resource; do
                    log_info "  - $resource"
                done

                # Remove cross-account resources from state
                echo "$state_resources" | while read -r resource; do
                    if [[ -n "$resource" ]]; then
                        log_action "Remove from state: $resource"
                        if tofu state rm "$resource" 2>/dev/null; then
                            log_success "Removed from state: $resource"
                        else
                            log_warn "Failed to remove from state: $resource"
                        fi
                    fi
                done
            else
                log_info "No cross-account resources found in Terraform state"
            fi

            # Also check for any orphaned modules
            local module_resources
            module_resources=$(tofu state list 2>/dev/null | grep -E "module\.(cross_account|cross-account)" || true)

            if [[ -n "$module_resources" ]]; then
                log_info "Found cross-account module resources in state:"
                echo "$module_resources" | while read -r resource; do
                    log_info "  - $resource"
                    log_action "Remove module from state: $resource"
                    if tofu state rm "$resource" 2>/dev/null; then
                        log_success "Removed module from state: $resource"
                    else
                        log_warn "Failed to remove module from state: $resource"
                    fi
                done
            fi

            popd >/dev/null 2>&1 || true
            log_success "Terraform state cleanup completed"
        fi
    fi
}

# Close member accounts (optional)
close_member_accounts() {
    log_info "🏢 Processing member account closure..."

    if [[ "$CLOSE_MEMBER_ACCOUNTS" != "true" ]]; then
        log_info "Member account closure disabled - skipping"
        return 0
    fi

    local current_account
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)

    # Only run from management account
    if [[ "$current_account" != "$MANAGEMENT_ACCOUNT_ID" ]]; then
        log_warn "Account closure only supported from management account"
        return 0
    fi

    # Verify we have organization access
    if ! aws organizations describe-organization >/dev/null 2>&1; then
        log_error "Unable to access AWS Organizations - cannot close member accounts"
        return 1
    fi

    log_warn "⚠️  ACCOUNT CLOSURE LIMITATIONS:"
    log_warn "   - Can only close 10% of member accounts within rolling 30-day period"
    log_warn "   - Closed accounts remain in organization for 90 days"
    log_warn "   - Outstanding fees and Reserved Instance charges still apply"
    log_warn "   - AWS Marketplace subscriptions must be manually canceled"

    # Map account IDs to environment names
    local -A account_env_map=(
        ["822529998967"]="Dev"
        ["927588814642"]="Staging"
        ["546274483801"]="Prod"
    )

    for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
        if ! check_account_filter "$account_id"; then
            log_info "Skipping account closure for $account_id - not in account filter"
            continue
        fi

        local env_name="${account_env_map[$account_id]}"

        # Check account status first
        local account_status
        account_status=$(aws organizations list-accounts --query "Accounts[?Id=='$account_id'].Status" --output text 2>/dev/null || echo "UNKNOWN")

        if [[ "$account_status" == "CLOSED" ]]; then
            log_info "Account $account_id ($env_name) is already closed - skipping"
            continue
        elif [[ "$account_status" == "UNKNOWN" ]]; then
            log_warn "Unable to determine status of account $account_id ($env_name) - skipping"
            continue
        fi

        log_warn "⚠️  About to close member account: $account_id ($env_name)"
        log_warn "   This action cannot be undone for 90 days"
        log_warn "   Ensure all critical resources have been backed up"

        if confirm_destruction "Member Account" "$account_id ($env_name) - PERMANENT CLOSURE"; then
            log_action "Close member account: $account_id ($env_name)"

            if [[ "$DRY_RUN" != "true" ]]; then
                # Note: AWS CLI doesn't currently support member account closure
                # This would need to be done via AWS Console or Organizations API
                log_warn "Member account closure must be performed manually via AWS Console"
                log_warn "Navigate to: AWS Organizations > Accounts > $account_id > Close account"
                log_warn "Enter account ID '$account_id' to confirm closure"

                # Future enhancement: Implement via AWS Organizations API when available
                # if aws organizations close-account --account-id "$account_id" 2>/dev/null; then
                #     log_success "Initiated closure of member account: $account_id ($env_name)"
                # else
                #     log_error "Failed to close member account: $account_id ($env_name)"
                # fi
            fi
        fi
    done

    if [[ "$DRY_RUN" != "true" ]]; then
        log_info "After manual account closure:"
        log_info "  - Accounts will show 'CLOSED' status for up to 90 days"
        log_info "  - Final bills will be generated for services used before closure"
        log_info "  - Reserved Instance charges will continue until expiration"
        log_info "  - You can reopen accounts during the 90-day period if needed"
    fi
}

# Cleanup orphaned resources that cost money
cleanup_orphaned_resources() {
    log_info "🧹 Scanning for orphaned resources that cost money..."

    # Unassociated Elastic IPs
    log_info "Checking for unassociated Elastic IPs..."
    local eips
    eips=$(timeout 30 aws ec2 describe-addresses --query 'Addresses[?AssociationId==null].{PublicIp:PublicIp,AllocationId:AllocationId}' --output json 2>/dev/null || echo "[]")

    if [[ "$eips" != "[]" ]] && [[ "$eips" != "null" ]] && [[ -n "$eips" ]]; then
        echo "$eips" | jq -c '.[]' | while read -r eip; do
            local public_ip allocation_id
            public_ip=$(echo "$eip" | jq -r '.PublicIp')
            allocation_id=$(echo "$eip" | jq -r '.AllocationId')

            if confirm_destruction "Orphaned Elastic IP" "$public_ip"; then
                log_action "Release Elastic IP: $public_ip"

                if [[ "$DRY_RUN" != "true" ]]; then
                    if aws ec2 release-address --allocation-id "$allocation_id" 2>/dev/null; then
                        log_success "Released Elastic IP: $public_ip"
                    else
                        log_error "Failed to release Elastic IP: $public_ip"
                    fi
                fi
            fi
        done
    fi
}

# Generate cost estimate
generate_cost_estimate() {
    log_info "💰 Generating monthly cost estimate for destroyed resources..."

    # This is a rough estimate based on typical AWS pricing
    local total_monthly_savings=0

    # Estimate savings (very rough)
    local s3_buckets_count
    s3_buckets_count=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null | wc -w 2>/dev/null || echo 0)
    s3_buckets_count=$(echo "$s3_buckets_count" | tr -d '[:space:]')
    [[ ! "$s3_buckets_count" =~ ^[0-9]+$ ]] && s3_buckets_count=0

    local cloudfront_count
    cloudfront_count=$(aws cloudfront list-distributions --query 'DistributionList.Items[].Id' --output text 2>/dev/null | wc -w 2>/dev/null || echo 0)
    cloudfront_count=$(echo "$cloudfront_count" | tr -d '[:space:]')
    [[ ! "$cloudfront_count" =~ ^[0-9]+$ ]] && cloudfront_count=0

    # Rough monthly cost estimates (in USD)
    local s3_cost=$((s3_buckets_count * 5))      # ~$5/month per bucket (very rough)
    local cloudfront_cost=$((cloudfront_count * 10))  # ~$10/month per distribution
    local dynamodb_cost=5                        # ~$5/month for state locking
    local kms_cost=10                            # ~$1/month per key + usage

    total_monthly_savings=$((s3_cost + cloudfront_cost + dynamodb_cost + kms_cost))

    log_success "Estimated monthly cost savings: \$${total_monthly_savings} USD"
    log_info "Note: This is a rough estimate. Actual costs depend on usage, data transfer, and storage."
}

# Generate comprehensive dry run report
generate_dry_run_report() {
    log_info "📋 Generating comprehensive dry run report..."

    local report_file="/tmp/destruction-report-$(date +%Y%m%d-%H%M%S).txt"
    local total_resources=0

    # Set timeout for long-running operations
    local AWS_CLI_TIMEOUT="timeout 10"

    {
        echo "==============================================="
        echo "AWS Infrastructure Destruction Report"
        echo "Generated: $(date)"
        echo "Account: $(aws sts get-caller-identity --query 'Account' --output text 2>/dev/null || echo 'Unknown')"
        echo "Region: $AWS_DEFAULT_REGION"
        echo "Cross-Account Mode: $INCLUDE_CROSS_ACCOUNT"
        echo "Member Account Closure: $CLOSE_MEMBER_ACCOUNTS"
        echo "Terraform State Cleanup: $CLEANUP_TERRAFORM_STATE"
        echo "==============================================="
        echo ""
        echo "RESOURCES THAT WOULD BE DESTROYED:"
        echo ""

        # Cross-Account Roles
        if [[ "$INCLUDE_CROSS_ACCOUNT" == "true" ]]; then
            echo "🔐 CROSS-ACCOUNT ROLES:"
            local current_account
            current_account=$(aws sts get-caller-identity --query 'Account' --output text 2>/dev/null)

            if [[ "$current_account" == "$MANAGEMENT_ACCOUNT_ID" ]]; then
                local -A account_env_map=(
                    ["822529998967"]="Dev"
                    ["927588814642"]="Staging"
                    ["546274483801"]="Prod"
                )

                local cross_account_count=0
                for account_id in "${MEMBER_ACCOUNT_IDS[@]}"; do
                    if check_account_filter "$account_id"; then
                        local env_name="${account_env_map[$account_id]}"
                        echo "  - GitHubActions-StaticSite-${env_name}-Role in account $account_id"
                        ((cross_account_count++)) || true
                    fi
                done
                echo "  Total: $cross_account_count cross-account roles"
                ((total_resources += cross_account_count)) || true
            else
                echo "  - Cross-account destruction requires management account access"
                echo "  Total: 0 cross-account roles (wrong account)"
            fi
            echo ""
        fi

        # S3 Buckets
        echo "🪣 S3 BUCKETS:"
        local buckets
        buckets=$($AWS_CLI_TIMEOUT AWS_DEFAULT_REGION=us-east-1 aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null || true)
        local bucket_count=0
        for bucket in $buckets; do
            if matches_project "$bucket"; then
                local size
                size=$(aws s3 ls "s3://$bucket" --recursive --summarize 2>/dev/null | grep "Total Size:" | cut -d: -f2 | xargs || echo "Unknown")
                echo "  - $bucket (Size: $size bytes)"
                ((bucket_count++)) || true
            fi
        done
        echo "  Total: $bucket_count buckets"
        ((total_resources += bucket_count)) || true
        echo ""

        # CloudFront Distributions
        echo "🌐 CLOUDFRONT DISTRIBUTIONS:"
        local distributions
        distributions=$($AWS_CLI_TIMEOUT aws cloudfront list-distributions --query 'DistributionList.Items[].{Id:Id,Comment:Comment,DomainName:DomainName}' --output json 2>/dev/null || echo "[]")
        local cf_count=0
        if [[ "$distributions" != "[]" ]] && [[ "$distributions" != "null" ]] && [[ -n "$distributions" ]]; then
            echo "$distributions" | jq -r '.[] | select(.Comment != null) | "  - " + .Id + " (" + .Comment + ") - " + .DomainName' | while read -r line; do
                if [[ -n "$line" ]]; then
                    echo "$line"
                    ((cf_count++)) || true
                fi
            done
            cf_count=$(echo "$distributions" | jq '. | length' 2>/dev/null || echo 0)
        else
            cf_count=0
        fi
        echo "  Total: $cf_count distributions"
        ((total_resources += cf_count)) || true
        echo ""

        # DynamoDB Tables
        echo "🗃️ DYNAMODB TABLES:"
        local tables
        tables=$(aws dynamodb list-tables --query 'TableNames[]' --output text 2>/dev/null || true)
        local table_count=0
        for table in $tables; do
            if matches_project "$table"; then
                echo "  - $table"
                ((table_count++)) || true
            fi
        done
        echo "  Total: $table_count tables"
        ((total_resources += table_count)) || true
        echo ""

        # KMS Keys
        echo "🔐 KMS KEYS:"
        local aliases
        aliases=$(aws kms list-aliases --query 'Aliases[].{AliasName:AliasName,TargetKeyId:TargetKeyId}' --output json 2>/dev/null || echo "[]")
        local kms_count=0
        if [[ "$aliases" != "[]" ]] && [[ "$aliases" != "null" ]] && [[ -n "$aliases" ]]; then
            echo "$aliases" | jq -c '.[]' | while read -r alias_info; do
            local alias_name
            alias_name=$(echo "$alias_info" | jq -r '.AliasName')
            if matches_project "$alias_name"; then
                echo "  - $alias_name"
                    ((kms_count++)) || true
                fi
            done
        fi
        echo "  Total: $kms_count keys"
        ((total_resources += kms_count)) || true
        echo ""

        # IAM Resources
        echo "👤 IAM RESOURCES:"
        local roles
        roles=$(aws iam list-roles --query 'Roles[].RoleName' --output text 2>/dev/null || true)
        local role_count=0
        echo "  Roles:"
        for role in $roles; do
            if matches_project "$role"; then
                echo "    - $role"
                ((role_count++)) || true
            fi
        done

        local policies
        policies=$(aws iam list-policies --scope Local --query 'Policies[].PolicyName' --output text 2>/dev/null || true)
        local policy_count=0
        echo "  Policies:"
        for policy in $policies; do
            if matches_project "$policy"; then
                echo "    - $policy"
                ((policy_count++)) || true
            fi
        done

        local oidc_count
        oidc_count=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[].Arn' --output text 2>/dev/null | grep -c "token.actions.githubusercontent.com" || echo 0)
        # Ensure oidc_count is a single number
        oidc_count=$(echo "$oidc_count" | head -1 | tr -d '[:space:]')
        [[ ! "$oidc_count" =~ ^[0-9]+$ ]] && oidc_count=0
        echo "  OIDC Providers: $oidc_count"
        echo "  Total: $((role_count + policy_count + oidc_count)) IAM resources"
        ((total_resources += role_count + policy_count + oidc_count)) || true
        echo ""

        # CloudWatch Resources
        echo "📊 CLOUDWATCH RESOURCES:"
        local log_groups
        log_groups=$(aws logs describe-log-groups --query 'logGroups[].logGroupName' --output text 2>/dev/null || true)
        local lg_count=0
        echo "  Log Groups:"
        for lg in $log_groups; do
            if matches_project "$lg" || [[ "$lg" == *"/aws/cloudtrail"* ]]; then
                echo "    - $lg"
                ((lg_count++)) || true
            fi
        done

        local alarms
        alarms=$(aws cloudwatch describe-alarms --query 'MetricAlarms[].AlarmName' --output text 2>/dev/null || true)
        local alarm_count=0
        echo "  Alarms:"
        for alarm in $alarms; do
            if matches_project "$alarm"; then
                echo "    - $alarm"
                ((alarm_count++)) || true
            fi
        done
        echo "  Total: $((lg_count + alarm_count)) CloudWatch resources"
        ((total_resources += lg_count + alarm_count)) || true
        echo ""

        # Summary
        echo "==============================================="
        echo "SUMMARY:"
        echo "  Total resources to be destroyed: $total_resources"
        echo "  Estimated monthly cost savings: ~\$$(generate_cost_estimate_value) USD"
        echo "==============================================="

    } | tee "$report_file"

    log_success "Dry run report saved to: $report_file"
    return 0
}

# Helper function for cost estimate value only
generate_cost_estimate_value() {
    local s3_buckets_count
    s3_buckets_count=$(aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null | wc -w || echo 0)

    local cloudfront_count
    cloudfront_count=$(aws cloudfront list-distributions --query 'DistributionList.Items[].Id' --output text 2>/dev/null | wc -w || echo 0)

    # Ensure numeric values
    s3_buckets_count=${s3_buckets_count:-0}
    cloudfront_count=${cloudfront_count:-0}

    # Convert to numeric if they contain whitespace
    s3_buckets_count=$(echo "$s3_buckets_count" | tr -d '[:space:]')
    cloudfront_count=$(echo "$cloudfront_count" | tr -d '[:space:]')

    # Ensure they're valid numbers
    [[ ! "$s3_buckets_count" =~ ^[0-9]+$ ]] && s3_buckets_count=0
    [[ ! "$cloudfront_count" =~ ^[0-9]+$ ]] && cloudfront_count=0

    local s3_cost=$((s3_buckets_count * 5))
    local cloudfront_cost=$((cloudfront_count * 10))
    local dynamodb_cost=5
    local kms_cost=10

    echo $((s3_cost + cloudfront_cost + dynamodb_cost + kms_cost))
}

# Parse command line arguments
parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --dry-run)
                DRY_RUN=true
                shift
                ;;
            --force)
                FORCE_DESTROY=true
                shift
                ;;
            --account-filter)
                ACCOUNT_FILTER="$2"
                shift 2
                ;;
            --region)
                AWS_DEFAULT_REGION="$2"
                shift 2
                ;;
            --no-cross-account)
                INCLUDE_CROSS_ACCOUNT=false
                shift
                ;;
            --close-accounts)
                CLOSE_MEMBER_ACCOUNTS=true
                shift
                ;;
            --no-terraform-cleanup)
                CLEANUP_TERRAFORM_STATE=false
                shift
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option: $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# Show help message
show_help() {
    cat << EOF
Usage: $SCRIPT_NAME [OPTIONS]

Destroy all AWS infrastructure created by the static-site repository including
cross-account resources, IAM/auth resources, and optionally member accounts.

OPTIONS:
    --dry-run                 Show what would be destroyed without actually doing it
    --force                   Skip all confirmation prompts (use with extreme caution)
    --account-filter IDS      Comma-separated list of AWS account IDs to limit destruction
    --region REGION           AWS region (default: us-east-1)
    --no-cross-account        Disable cross-account role destruction
    --close-accounts          Enable member account closure (PERMANENT)
    --no-terraform-cleanup    Disable Terraform state cleanup
    -h, --help               Show this help message

CROSS-ACCOUNT FEATURES:
    • Destroys GitHub Actions roles across all member accounts
    • Cleans up cross-account Terraform state
    • Requires management account access (${MANAGEMENT_ACCOUNT_ID})
    • Uses OrganizationAccountAccessRole for cross-account access

MEMBER ACCOUNTS:
    • Dev Account:     822529998967
    • Staging Account: 927588814642
    • Prod Account:    546274483801

EXAMPLES:
    # Dry run to see what would be destroyed (recommended first)
    $SCRIPT_NAME --dry-run

    # Complete infrastructure destruction including cross-account
    $SCRIPT_NAME --force

    # Destroy only specific accounts
    $SCRIPT_NAME --account-filter "822529998967,927588814642" --dry-run

    # Full destruction including member account closure (EXTREME)
    $SCRIPT_NAME --force --close-accounts

    # Disable cross-account features
    $SCRIPT_NAME --dry-run --no-cross-account

    # Cleanup current account only, no state cleanup
    $SCRIPT_NAME --no-cross-account --no-terraform-cleanup

ENVIRONMENT VARIABLES:
    AWS_DEFAULT_REGION        AWS region (default: us-east-1)
    FORCE_DESTROY            Set to 'true' to skip confirmations
    DRY_RUN                  Set to 'true' for dry run mode
    ACCOUNT_FILTER           Comma-separated AWS account IDs
    INCLUDE_CROSS_ACCOUNT    Set to 'false' to disable cross-account destruction
    CLOSE_MEMBER_ACCOUNTS    Set to 'true' to enable account closure
    CLEANUP_TERRAFORM_STATE  Set to 'false' to disable state cleanup

DESTRUCTION PHASES:
    Phase 1: Cross-account infrastructure cleanup
    Phase 2: Dependent resources (CloudFront, WAF)
    Phase 3: Storage and logging (S3, CloudTrail, CloudWatch, SNS)
    Phase 4: Compute and database (DynamoDB)
    Phase 5: Identity and security (IAM, KMS)
    Phase 6: Orphaned resources cleanup
    Phase 7: Member account closure (if enabled)

SAFETY FEATURES:
    • Dry run mode shows complete destruction plan
    • Individual confirmation for each resource type
    • Account filtering to limit scope
    • Enhanced logging and progress tracking
    • Cross-account access validation

WARNING - PERMANENT DATA LOSS:
    This script will PERMANENTLY DELETE all matching AWS resources including:
    • All S3 buckets and contents
    • All IAM roles, policies, and OIDC providers (including cross-account)
    • All KMS keys (scheduled for deletion)
    • All CloudFront distributions and associated resources
    • All DynamoDB tables and Terraform state locks
    • Terraform state for cross-account modules
    • Optionally: Member accounts (90-day closure period)

    USE --dry-run FIRST to review the complete destruction plan.
    Member account closure cannot be undone for 90 days.
EOF
}

# Main execution function
main() {
    local start_time
    start_time=$(date +%s)

    # Only show banner if not in force mode
    if [[ "$FORCE_DESTROY" != "true" ]]; then
        echo -e "${BOLD}${RED}"
        echo "╔══════════════════════════════════════════════════════════════╗"
        echo "║                    🚨 DANGER ZONE 🚨                        ║"
        echo "║                                                              ║"
        echo "║  This script will PERMANENTLY DELETE all AWS resources      ║"
        echo "║  created by the static-site infrastructure repository.      ║"
        echo "║                                                              ║"
        echo "║  Resources that will be destroyed:                          ║"
        echo "║  • S3 buckets and all contents                              ║"
        echo "║  • KMS keys (scheduled for deletion)                       ║"
        echo "║  • IAM roles, policies, and OIDC providers                 ║"
        echo "║  • CloudFront distributions                                 ║"
        echo "║  • DynamoDB tables                                          ║"
        echo "║  • CloudTrail trails and logs                              ║"
        echo "║  • All other project-related AWS resources                 ║"
        echo "║                                                              ║"
        echo "║  💸 This action may result in significant cost savings      ║"
        echo "║  💀 This action CANNOT be undone                           ║"
        echo "╚══════════════════════════════════════════════════════════════╗"
        echo -e "${NC}"
    fi

    log_info "Starting infrastructure destruction script"
    log_info "Log file: $LOG_FILE"
    log_info "Dry run mode: $DRY_RUN"
    log_info "Force mode: $FORCE_DESTROY"
    log_info "AWS Region: $AWS_DEFAULT_REGION"

    if [[ -n "$ACCOUNT_FILTER" ]]; then
        log_info "Account filter: $ACCOUNT_FILTER"
    fi

    # Verify AWS CLI is configured
    if ! aws sts get-caller-identity >/dev/null 2>&1; then
        log_error "AWS CLI is not configured or lacks permissions"
        log_error "Please run 'aws configure' or set up AWS credentials"
        exit 3
    fi

    local current_account current_region
    current_account=$(aws sts get-caller-identity --query 'Account' --output text)
    current_region=$(aws configure get region || echo "$AWS_DEFAULT_REGION")

    log_info "Current AWS Account: $current_account"
    log_info "Current AWS Region: $current_region"

    # Final confirmation
    if [[ "$FORCE_DESTROY" != "true" ]] && [[ "$DRY_RUN" != "true" ]]; then
        echo
        echo -e "${RED}${BOLD}FINAL WARNING:${NC}"
        echo "You are about to destroy ALL infrastructure in AWS account: $current_account"
        echo "This includes PERMANENT deletion of data and resources."
        echo
        read -p "Type 'DESTROY EVERYTHING' to confirm: " final_confirmation

        if [[ "$final_confirmation" != "DESTROY EVERYTHING" ]]; then
            log_warn "Operation cancelled by user"
            exit 2
        fi
    fi

    # If dry run, generate report and exit
    if [[ "$DRY_RUN" == "true" ]]; then
        log_info "Running in DRY RUN mode - no resources will be destroyed"
        generate_dry_run_report

        local end_time duration
        end_time=$(date +%s)
        duration=$((end_time - start_time))

        log_success "Dry run completed in ${duration} seconds"
        log_info "Review the report above to see what would be destroyed"
        log_info "To perform actual destruction, run without --dry-run"
        exit 0
    fi

    log_info "Beginning destruction sequence..."

    # Track destruction results
    declare -A destruction_results
    local total_destroyed=0
    local total_failed=0

    # Execute destruction in order (dependent resources first)
    log_info "Phase 1: Cross-account infrastructure cleanup..."
    destroy_cross_account_roles
    cleanup_terraform_state

    log_info "Phase 2: Destroying dependent resources..."
    destroy_cloudfront_distributions
    destroy_waf_resources

    log_info "Phase 3: Destroying storage and logging..."
    destroy_s3_buckets
    destroy_cloudtrail_resources
    destroy_cloudwatch_resources
    destroy_sns_resources

    log_info "Phase 4: Destroying compute and database resources..."
    destroy_dynamodb_tables

    log_info "Phase 5: Destroying identity and security..."
    destroy_iam_resources
    destroy_kms_keys

    log_info "Phase 6: Cleanup orphaned resources..."
    cleanup_orphaned_resources

    log_info "Phase 7: Member account closure (if enabled)..."
    close_member_accounts

    # Generate cost savings estimate
    generate_cost_estimate

    local end_time duration
    end_time=$(date +%s)
    duration=$((end_time - start_time))

    log_success "Infrastructure destruction completed in ${duration} seconds"
    log_success "Log file saved: $LOG_FILE"

    echo
    log_success "🎉 All infrastructure has been destroyed!"
    log_success "💰 You should see cost savings on your next AWS bill"
    echo
    log_warn "Note: Some resources like KMS keys have mandatory waiting periods"
    log_warn "Check the AWS console to verify all resources are gone"

    # Generate final summary
    echo
    echo -e "${BOLD}${GREEN}DESTRUCTION SUMMARY:${NC}"
    echo "  Total resources destroyed: ${total_destroyed:-unknown}"
    echo "  Failed destructions: ${total_failed:-0}"
    echo "  Log file: $LOG_FILE"
}

# Script entry point
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    # Parse command line arguments
    parse_arguments "$@"

    # Run main function
    main
fi